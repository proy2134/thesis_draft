\chapter{Related Work} \label{chapter:relwork}
In this chapter, we position this dissertation to relevant literature. We break this chapter into three sections. In Section~\ref{sec:relwork_yield}, we discuss existing techniques for overall yield mapping, fruit detection, counting, and tracking. In Section~\ref{sec:relwork_active}, we discuss relevant work on active perception for precision farming and phenotyping. Finally, in section~\ref{sec:relwork_semgan}, we discuss the relevant literature for image to image translation and domain transfer.


\section{Relevant literature for yield mapping systems and sub-modules}\label{sec:relwork_yield} 
 
The variability in plant size, color, shape, etc., make it difficult to develop a general yield mapping method for all specialty crops. Instead, researchers have focused on developing yield mapping systems specific to each crop. In this dissertation, we mainly focus on apple orchards. However, as part of relevant literature, we discuss yield estimation systems and components for fruit similar to apples such as citrus, pomegranates, tomatoes, etc.

There has been a significant recent activity for automating yield estimation(\cite{wang, das2015devices, hung2015feature,gongal2016apple}) for specialty crops. While some of these existing works focus on the entire yield estimation systems, others focus on specific components only. In this section, we first discuss complete yield estimation systems and then focus on individual components.

Wang et al.~\cite{wang} presented a complete system for apple yield estimation. Their system used a side facing, wide-baseline vertical stereo rig mounted on an autonomous ground vehicle. The system operated in controlled artificial illumination settings at night. It uses flashlights to illuminate the apples and classified them using HSV~\cite{hsvcolor} color thresholds. In addition to count information, the stereo system extracted fruit sizes. Hung et al.~\cite{hung2015feature} presented a feature-based learning approach for the identification of red and green apples and the extraction of fruit count. They used a conditional random field classifier to segment fruits using color and depth data. Das et al.~\cite{das2015devices} developed a sensor suite for extracting plant morphology, canopy volume, leaf area index and fruit counts that consists of a laser range scanner, multi-spectral cameras, a thermal imaging camera, and navigational sensors. They used a Support Vector Machine (SVM) trained on pixel color to locate apples in images. Gongal et al.~\cite{gongal2016apple}  developed a new sensor system with an over-the-row platform integrated with a tunnel structure that acquired images from opposite sides of apple trees. The tunnel structure is used to minimize illumination of apples with direct sunlight and to reduce the variability in lighting. In contrast to these systems, our goal is to develop a general computer vision system to extract the count of apples in an orchard row from a monocular camera. Our developed system is independent of any particular hardware platform and can be used with various types of robotic and non-robotic systems. In the rest of this section, we will discuss systems developed for tackling specific portions of the yield estimation system.

\subsection{Fruit Detection} The problem of locating fruits from captured photographs have been studied extensively in the literature. Early systems relied on hard color thresholds. Early methods relying on machine learning were targeted to learn these thresholds. Jimenez et al.~\cite{jimenez2000survey} presented a survey of these early computer vision methods for locating fruits on trees. More recent approaches include Zhou et al~\cite{zhouGala}, who presented a method that uses RGB and HSV color thresholds to detect Gala apples. Linker et al.~\cite{Linker}, used K-nearest neighbor (KNN) classifier along with blob and arc fitting to find red and green apples. The authors avoid specular lighting conditions by capturing images close to sunset. Changyi et al.~\cite{changyi2015apple} developed an apple detection method that uses the backpropagation neural network (\cite{hecht1989theory}) trained on color features. Similar to them, Liu et al.~\cite{liu2016method} presented a method for detecting apples at night using pixel color and position as features in neural networks. The earliest work close to our semi-supervised color based clustering method is Tabb et al.~\cite{tabb2006segmentation}. The authors developed a method for detecting apples from videos using background modeling by global Gaussian mixture models (GMM). The method relied on images collected using an over-the-row harvester platform that provides a consistent background and illumination setting. 



Recent advancements in deep learning inspired researchers to apply these techniques for identifying apples. Bargoti et al.~\cite{bargoti2017image} used the Multi-Scale Multi-Layered Perceptron network (MLP) for apple and almond detection. Stein et al.~\cite{stein2016image} used a similar deep learning technique to identify mangoes. Chen et al.~\cite{chen2017counting} used a Fully Convolutional Neural Network (\cite{long2015fully}) for apple segmentation.  In 2015, Faster R-CNN~\cite{ren_faster_2015} (FRCNN) became the de-facto state-of-the-art object detection method.  Many recent papers used this network for fruit and vegetable detection. \cite{bargoti_deep_2017} used an FRCNN to detect and count almonds, mangoes, and apples, achieving $F_1$-scores of $>0.9$. \cite{sa_deepfruits:_2016} used a similar approach to detect green peppers. They merged RGB and Near-Infrared (NIR) data and achieved $F_1$-scores of $0.84$. In contrast to these methods, in this dissertation, we develop a semi-supervised color based clustering method for detecting fruit. Additionally, we present a fully supervised method for fruit detection which treats it as a pixel-wise classification problem. We perform a rigorous comparison of these two methods a state-of-the-art object detection approach based on Faster R-CNN~\cite{bargoti_deep_2017}.

A key observation we had is that: though deep learning methods are accurate, they require a large amount of training data [e.g we trained a Fully Convolutional Network (FCN) using the data from (\cite{bargoti2017image}) and the network did not perform well on our data. When we use some of our data for training though, performance improved drastically]. Generating such training data for different varieties of fruits, in different lighting conditions are tedious and cumbersome. One of the main objectives of this dissertation is to develop techniques to generate realistic synthetic data to alleviate this problem.

\subsection{Counting Fruits} Early methods for counting circular fruit such as apple, citrus, etc. from segmented images are dominated by circular Hough transforms (CHT) \cite{silwal2014apple,changyi2015apple,liu2016method}. The main bottleneck of using CHT is that the parameters need to be tuned across different datasets. Another issue with this method is occlusion. Due to occlusion fruit is not fully visible and cannot be approximated by a circle. Different from these methods, Senthilnath et al.~\cite{senthilnath2016detection} used  K-means, a mixture of Gaussians and Self-Organizing Maps (SOM) to detect individual tomatoes within a cluster. They used the Bayesian Information Criterion (BIC) (\cite{bic}) to select the optimal number of components in these methods. Detection methods using state-of-the-art object detection networks do not require additional counting algorithms. Instead, the network outputs are summed up to compute counts~\cite{bargoti_deep_2017, sa_deepfruits:_2016, stein_image_2016}. Object detection networks use Non-Maximum Suppression (NMS) to reject overlapping instances, and often true positives are filtered out. \cite{bargoti_deep_2017} found that up to $4\%$ of their error was due to this filtering process. 

Alternatively, \cite{chen_counting_2017}, used a combination of two networks for fruit counting. First, an FCN creates feature maps of possible targets in the image. Second, a CNN counts these targets using a regression head.\cite{maryam_rahnemoonfar_deep_2017} used a CNN for direct counting of red fruits from simulated data. They used synthetic data to train a network on $728$ images to count red tomatoes. 

In contrast, in this dissertation, we present an unsupervised clustering approach based on the Gaussian mixture model (GMM) to count fruit from registered and segmented fruit clusters. Unlike CHT, our method does not require any parameter tuning and can handle arbitrarily complex clusters. Additionally, we perform a rigorous comparison with one of our recent work~\cite{hani_apple_2018,hani_jfr_counting} that formulates fruit counting as a multi-class classification problem and solves it using a deep residual network ResNet~\cite{he_deep_2015}. 


\subsection{Fruit Tracking} Compared to detecting and counting fruit in images, the studies on tracking fruit across multiple frames have been limited. To obtain an accurate yield estimate, in addition to tracking fruits from a single side of the row, we need to account for fruit visible from both sides.

Most of the previously mentioned yield estimation pipelines studied the single-side tracking problem. Wang et al.~\cite{wang} used stereo cameras and point cloud alignment (using odometry and GPS) to track fruit. They aligned the apples in 3D space and removed the ones which are within five centimeters of a previously registered apple. Hung et al.~\cite{hung2015feature} used sampling at certain intervals to remove the overlap between images. Das et al.~\cite{das2015devices} used optical flow and navigational sensors to avoid duplicate apples. Liu et al.~\cite{liu_robust_2018} used the Hungarian algorithm~\cite{kuhn1955hungarian} with an objective cost computed from using a Kalman filter~\cite{kalman} corrected KLT tracker~\cite{klt}. They localize the fruit in 3D using a Structure from Motion (SfM) algorithm that tracks SIFT features that are located near the fruit regions. Stein et al.~\cite{stein_image_2016} performed frame-to-frame fruit tracking using epipolar geometry combined with the Hungarian algorithm for optimal tracking assignment. In contrast to these methods, we developed an incremental SfM pipeline for tracking. Our method utilizes fruit themselves as features and builds a dense reconstruction of the entire row. Afterward, tracking is accomplished by reprojecting the 3D apple clusters on individual images. 


 To eliminate fruit visible from both sides, global point cloud alignment using odometry and GPS has been tried~\cite{wang}. However, in orchard environments, good features cannot be stably tracked through long subsequent frames because of motion due to the wind in the scene. Accumulated errors in camera pose lead to misalignment. State-of-the-art methods for volumetric fusion~\cite{newcombe2011kinectfusion}, SfM~\cite{wu2013towards} and SLAM~\cite{mur2017orb} are not reliable enough for eliminating visible fruit from both sides. Since there is nearly no overlap of canopy surface between two sides of tree rows, misalignment of tree models cannot be addressed by ICP-based methods~\cite{medeiros2017modeling} or semantic tracking in loop closure~\cite{bowman2017probabilistic}.
 
 Generated two-sides point clouds can also be manually matched through CAD software~\cite{rosell2009obtaining}. However, tree models are partially misaligned from two sides due to accumulated errors of sensor poses during the movement. Even if position accuracy has been improved by combining Global Navigation Satellite System (GNSS) with LIDAR~\cite{del2015georeferenced}, the issue of accumulated orientation error still exists, especially for large scale scanning. Furthermore, the combination of these two sensors (e.g. GR3 RTK GNSS and LMS500) is expensive and may not be affordable.
 
 
 Another approach to solving this problem is to merge the independent 3D reconstructions from both sides using visual features only. Recent studies present methods for merging visually disconnected SfM models~\cite{cohen2016indoor} for urban environments. These methods estimate the scale and relative height for all sub-models based on the Manhattan world assumption and find possible connection points between the reconstructions utilizing semantic information (facade edges, windows, doors, etc). Afterward, they generate all possible fully connected models and find the most likely model according to loop closure and symmetry alignment. Modern map merging approaches~\cite{zhou2016fast,bonanni20173} register multiple partially overlapping 3D maps by non-rigid alignment. In contrast to these methods, for aligning SfM reconstructions from both sides of a row, we utilize the symmetry of their occlusion boundaries from a fronto-parallel view. Subsequently, we adjust the overlap in-depth direction using semantic information. This brings us to the end of the discussion for relevant literature for yield mapping. For the rest of this chapter, we discuss the relevant literature for active view planning for fruit counting and image to image translation.

\section{Active Perception}\label{sec:relwork_active}
In active vision, the parameters of a camera are actively controlled to improve the performance of the vision system~\cite{bajcsy1988active}. One of the fundamental problems in active vision is to view planning to improve coverage ~\cite{scott2003view}. Recently, there has been increasing activity to scale up active perception of sensing tasks over longer periods and in larger environments~\cite{bajcsy2016revisiting}. Recent work in this area is concerned with object detection and classification~\cite{atanasov2014nonmyopic,potthast2014probabilistic,pattenmonte}. 
In the context of agricultural applications, most of the work on yield mapping does not involve active perception~\cite{hung2015feature, wang, das2015devices, deepApple, roy2016counting, roy2016surveying, peng2016semantic}. Relevant work includes Baur et al.~\cite{baur2014path} designed a heuristic-based algorithm for trajectory generation for apple picking. In contrast to this work, in chapter~\ref{chapter:active_counting} we explicitly address the occlusions among objects and present representations and planning algorithms to estimate their count.




\section{Semantics-aware image to image translation}\label{sec:relwork_semgan}
As we mentioned before in Chapter~\ref{chapter:intro}, deep learning methods are data-hungry. Obtaining such data for many settings such as specialty crops is tedious and cumbersome. To alleviate this problem, we have designed a framework that can leverage unsupervised image to image translation techniques to generate realistic synthetic data (See Chapter~\ref{chapter:semgan}). Here we discuss the relevant literature which has influenced our method.


\subsection{Generative Adversarial Networks (GAN's)}~\cite{goodfellow_generative_2014} have produced high-quality results on a variety of tasks. These type of network has been used extensively for image to image translation, both in a supervised~\cite{eigen_predicting_2015, isola_image--image_2016, wang_video--video_2018, xian_texturegan:_2018} and unsupervised~\cite{taigman_unsupervised_2016, zhu_unpaired_2017, yi_dualgan:_2017, gonzalez-garcia_image--image_2018, benaim_one-sided_2017, zhang_harmonic_2019, amodio_travelgan:_2019} fashion. The key to their success is the inclusion of an adversarial loss, which drives the generator part of the network to create indistinguishable samples to the target distribution. However, the adversarial loss not monotonic and often lead to mode collapse~\cite{arjovsky2017wasserstein}. CycleGAN~\cite{zhu_unpaired_2017} introduced a cyclic consistency term to regularize the loss function and avoid mode collapse providing a stable framework for training GANs.

\subsection{Unsupervised image to image translation with semantic information}
The idea to use semantic consistency to translate between the source domain and target domain has been studied in the literature. Li et al.~\cite{li_semantic-aware_2018} extended the CycleGAN framework with a Sobel filter loss. The Sobel filter is convolved with the semantic label in the source domain and the translated image to preserve the semantic boundaries between them. Ramirez et al.~\cite{ramirez_exploiting_2018} proposed to extend the discriminator network of CycleGAN to contain a classification and semantic segmentation network. The segmentation network is trained ahead of time and is integrated into the discriminator during the training stage. This idea of using a tasks specific network, either for classification or semantic segmentation, can also be found in Cycada~\cite{hoffman_cycada:_2017}. Here, the network adds a task-specific loss to penalize semantic inconsistencies between the translated image and the source image. Sem-GAN~\cite{cherian_sem-gan:_2018} proposed two semantic segmentation functions that are trained on the respective domains. The output of the generator is used as input to this semantic segmentation function which again penalizes semantic inconsistency.
In contrast to our method, these works rely heavily on the introduction of sizeable semantic segmentation networks, which is computationally expensive. Additionally, they restrict the geometric changes that a network is allowed to perform. When a translation requires substantial geometric changes, as in Sheep $\leftrightarrows$ Giraffe, this is not desirable.

\subsection{Object transfiguration and semantic manipulation}
Most of the image to image translation methods discussed so far are only capable of modifying low-level content of images, such as transferring colors or textures. They fail to perform large semantic changes between objects (e.g. transform sheep into giraffes). This failure is due to the underlying assumption that the scenes and the contained objects are similar in geometric composition across both domains. Others have focused on using GAN's for object transfiguration and semantic manipulation. One proposed method~\cite{liang_generative_2017} is to optimize a conditional generator and several semantic-aware discriminators. Another idea~\cite{alami_mejjati_unsupervised_2018} relied on modeling an attention map to extract the foreground objects. Instead of considering the whole image in the transfiguration task, the generator is restricted to the foreground objects. InstaGAN~\cite{mo_instance-aware_2019} proposed to use instance-level segmentation masks in both, the source and the target domains. Each of the generator and discriminator networks receives an image and a set of instance masks, which are then translated one by one. To preserve the background, they introduce a context preservation loss. 

In Chapter~\ref{chapter:semgan}, we show that we can obtain superior performance on the object transfiguration task using only a slight modification to the original CycleGAN approach. We present an encoder-decoder based generator architecture that can translate both the input image and corresponding semantic map to the target domain in a single forward pass of the network.

\subsection{Domain adaptation}
In general, networks trained on a source domain and tested on a target domain will perform poorly, even if the two domains appear visually similar to an outside observer. Domain adaptation tries to bridge this gap. This topic has been extremely popular in the last few years~\cite{hoffman_cycada:_2017, murez_image_2017, hong_conditional_2018, zhang_collaborative_2018, shen_wasserstein_2017, hassan_unsupervised_2018, long_conditional_2018, hosseini-asl_augmented_2019}. While these approaches can translate from one image domain to another, the underlying semantics are lost if the network is allowed to manipulate an object's geometry. In Chapter~\ref{chapter:semgan}, we show that we can perform domain adaptation while preserving these semantic label maps.


We have presented a high-level problem definition and overview of the designed algorithms in Chapter~\ref{chapter:intro} and surveyed existing approaches in this chapter. We now move on to the main results of the dissertation. We start with the fruit detection techniques in the next chapter.
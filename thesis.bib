
@article{stein_image_2016,
	title = {Image {Based} {Mango} {Fruit} {Detection}, {Localisation} and {Yield} {Estimation} {Using} {Multiple} {View} {Geometry}},
	volume = {16},
	issn = {1424-8220},
	doi = {10.3390/s16111915},
	language = {en},
	number = {11},
	urldate = {2017-08-26},
	journal = {Sensors},
	author = {Stein, Madeleine and Bargoti, Suchet and Underwood, James},
	month = nov,
	year = {2016},
	pages = {1915},
	file = {sensors-16-01915.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/HGJ4BC3I/sensors-16-01915.pdf:application/pdf}
}

@article{bargoti_image_2017,
	title = {Image segmentation for fruit detection and yield estimation in apple orchards},
	urldate = {2017-08-26},
	journal = {Journal of Field Robotics},
	author = {Bargoti, Suchet and Underwood, James P.},
	year = {2017},
	file = {1610.08120.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/KDBW322S/1610.08120.pdf:application/pdf}
}

@article{sa_deepfruits:_2016,
	title = {{DeepFruits}: {A} {Fruit} {Detection} {System} {Using} {Deep} {Neural} {Networks}},
	volume = {16},
	issn = {1424-8220},
	shorttitle = {{DeepFruits}},
	doi = {10.3390/s16081222},
	language = {en},
	number = {8},
	urldate = {2017-08-26},
	journal = {Sensors},
	author = {Sa, Inkyu and Ge, Zongyuan and Dayoub, Feras and Upcroft, Ben and Perez, Tristan and McCool, Chris},
	month = aug,
	year = {2016},
	pages = {1222},
	file = {sensors-16-01222-v2.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/YIHGW6K4/sensors-16-01222-v2.pdf:application/pdf}
}

@inproceedings{bargoti_deep_2017,
	title = {Deep fruit detection in orchards},
	urldate = {2017-08-26},
	booktitle = {Robotics and {Automation} ({ICRA}), 2017 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Bargoti, Suchet and Underwood, James},
	year = {2017},
	pages = {3626--3633},
	file = {1610.03677.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/UCPX579J/1610.03677.pdf:application/pdf}
}

@incollection{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {91--99}
}

@article{maryam_rahnemoonfar_deep_2017,
	title = {Deep {Count}: {Fruit} {Counting} {Based} on {Deep} {Simulated} {Learning}},
	volume = {17},
	issn = {1424-8220},
	shorttitle = {Deep {Count}},
	url = {http://www.mdpi.com/1424-8220/17/4/905},
	doi = {10.3390/s17040905},
	language = {en},
	number = {12},
	urldate = {2018-02-27},
	journal = {Sensors},
	author = {{Maryam Rahnemoonfar} and {Clay Sheppard}},
	month = apr,
	year = {2017},
	pages = {905}
}

@inproceedings{roy_registering_2018,
	title = {Registering {Reconstructions} of the {Two} {Sides} of {Fruit} {Tree} {Rows}},
	booktitle = {Intelligent {Robots} and {Systems} ({IROS}), 2018 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Roy, Pravakar and Dong, Wenbo and Isler, Volkan},
	year = {2018},
	annote = {Submission Number 1854}
}

@inproceedings{roy_surveying_2016,
	title = {Surveying apple orchards with a monocular vision system},
	isbn = {978-1-5090-2409-4},
	url = {http://ieeexplore.ieee.org/document/7743500/},
	doi = {10.1109/COASE.2016.7743500},
	urldate = {2018-07-04},
	booktitle = {International {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	publisher = {IEEE},
	author = {Roy, Pravakar and Isler, Volkan},
	month = aug,
	year = {2016},
	pages = {916--921},
	file = {07743500.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/AQNSMYET/07743500.pdf:application/pdf}
}

@incollection{kulic_vision-based_2017,
	address = {Cham},
	title = {Vision-{Based} {Apple} {Counting} and {Yield} {Estimation}},
	volume = {1},
	isbn = {978-3-319-50114-7 978-3-319-50115-4},
	url = {http://link.springer.com/10.1007/978-3-319-50115-4_42},
	urldate = {2018-07-04},
	booktitle = {2016 {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Roy, Pravakar and Isler, Volkan},
	editor = {Kulić, Dana and Nakamura, Yoshihiko and Khatib, Oussama and Venture, Gentiane},
	year = {2017},
	doi = {10.1007/978-3-319-50115-4_42},
	pages = {478--487}
}

@inproceedings{ronneberger_u-net:_2015,
	title = {U-net: {Convolutional} networks for biomedical image segmentation},
	shorttitle = {U-net},
	booktitle = {International {Conference} on {Medical} image computing and computer-assisted intervention},
	publisher = {Springer},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	pages = {234--241},
	file = {1505.04597.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/88MBWTYG/1505.04597.pdf:application/pdf}
}

@incollection{desai_automated_2013,
	address = {Heidelberg},
	title = {Automated {Crop} {Yield} {Estimation} for {Apple} {Orchards}},
	volume = {88},
	isbn = {978-3-319-00064-0 978-3-319-00065-7},
	url = {http://link.springer.com/10.1007/978-3-319-00065-7_50},
	urldate = {2018-07-05},
	booktitle = {Experimental {Robotics}},
	publisher = {Springer International Publishing},
	author = {Wang, Qi and Nuske, Stephen and Bergerman, Marcel and Singh, Sanjiv},
	editor = {Desai, Jaydev P. and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},
	year = {2013},
	doi = {10.1007/978-3-319-00065-7_50},
	pages = {745--758},
	file = {2012iserApple.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/5Y8PRXPE/2012iserApple.pdf:application/pdf}
}

@incollection{mejias_feature_2015,
	address = {Cham},
	title = {A {Feature} {Learning} {Based} {Approach} for {Automated} {Fruit} {Yield} {Estimation}},
	volume = {105},
	isbn = {978-3-319-07487-0 978-3-319-07488-7},
	url = {http://link.springer.com/10.1007/978-3-319-07488-7_33},
	urldate = {2018-07-05},
	booktitle = {Field and {Service} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Hung, Calvin and Underwood, James and Nieto, Juan and Sukkarieh, Salah},
	editor = {Mejias, Luis and Corke, Peter and Roberts, Jonathan},
	year = {2015},
	doi = {10.1007/978-3-319-07488-7_33},
	pages = {485--498},
	file = {482e8a199a6bd9afb272db23556a6c8ae155.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/9YH6KHLP/482e8a199a6bd9afb272db23556a6c8ae155.pdf:application/pdf}
}

@inproceedings{das_devices_2015,
	title = {Devices, systems, and methods for automated monitoring enabling precision agriculture},
	isbn = {978-1-4673-8183-3},
	url = {http://ieeexplore.ieee.org/document/7294123/},
	doi = {10.1109/CoASE.2015.7294123},
	urldate = {2018-07-05},
	publisher = {IEEE},
	author = {Das, Jnaneshwar and Cross, Gareth and Qu, Chao and Makineni, Anurag and Tokekar, Pratap and Mulgaonkar, Yash and Kumar, Vijay},
	month = aug,
	year = {2015},
	pages = {462--469},
	file = {07294123.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/KIMZ2SBV/07294123.pdf:application/pdf}
}

@article{chen_counting_2017,
	title = {Counting {Apples} and {Oranges} {With} {Deep} {Learning}: {A} {Data}-{Driven} {Approach}},
	volume = {2},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Counting {Apples} and {Oranges} {With} {Deep} {Learning}},
	url = {http://ieeexplore.ieee.org/document/7814145/},
	doi = {10.1109/LRA.2017.2651944},
	number = {2},
	urldate = {2018-07-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chen, Steven W. and Shivakumar, Shreyas S. and Dcunha, Sandeep and Das, Jnaneshwar and Okon, Edidiong and Qu, Chao and Taylor, Camillo J. and Kumar, Vijay},
	month = apr,
	year = {2017},
	pages = {781--788},
	file = {07814145.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/RJBHK3LH/07814145.pdf:application/pdf}
}

@article{gongal_apple_2016,
	title = {Apple crop-load estimation with over-the-row machine vision system},
	volume = {120},
	issn = {01681699},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S016816991500335X},
	doi = {10.1016/j.compag.2015.10.022},
	language = {en},
	urldate = {2018-07-05},
	journal = {Computers and Electronics in Agriculture},
	author = {Gongal, A. and Silwal, A. and Amatya, S. and Karkee, M. and Zhang, Q. and Lewis, K.},
	month = jan,
	year = {2016},
	pages = {26--35},
	file = {1-s2.0-S016816991500335X-main.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/IQGM2NTZ/1-s2.0-S016816991500335X-main.pdf:application/pdf}
}

@article{otsu_threshold_1979,
	title = {A {Threshold} {Selection} {Method} from {Gray}-{Level} {Histograms}},
	volume = {9},
	issn = {0018-9472, 2168-2909},
	url = {http://ieeexplore.ieee.org/document/4310076/},
	doi = {10.1109/TSMC.1979.4310076},
	number = {1},
	urldate = {2018-07-28},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Otsu, Nobuyuki},
	month = jan,
	year = {1979},
	pages = {62--66}
}

@techreport{achanta_slic_2010,
	title = {Slic superpixels},
	author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and Süsstrunk, Sabine},
	year = {2010},
	file = {SLIC_Superpixels_TR_2.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/GTSA9M8B/SLIC_Superpixels_TR_2.pdf:application/pdf}
}

@article{gongal_sensors_2015,
	title = {Sensors and systems for fruit detection and localization: {A} review},
	volume = {116},
	issn = {01681699},
	shorttitle = {Sensors and systems for fruit detection and localization},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0168169915001581},
	doi = {10.1016/j.compag.2015.05.021},
	language = {en},
	urldate = {2018-07-28},
	journal = {Computers and Electronics in Agriculture},
	author = {Gongal, A. and Amatya, S. and Karkee, M. and Zhang, Q. and Lewis, K.},
	month = aug,
	year = {2015},
	pages = {8--19}
}

@article{liu_robust_2018,
	title = {Robust {Fruit} {Counting}: {Combining} {Deep} {Learning}, {Tracking}, and {Structure} from {Motion}},
	shorttitle = {Robust {Fruit} {Counting}},
	url = {http://arxiv.org/abs/1804.00307},
	abstract = {We present a novel fruit counting pipeline that combines deep segmentation, frame to frame tracking, and 3D localization to accurately count visible fruits across a sequence of images. Our pipeline works on image streams from a monocular camera, both in natural light, as well as with controlled illumination at night. We ﬁrst train a Fully Convolutional Network (FCN) and segment video frame images into fruit and non-fruit pixels. We then track fruits across frames using the Hungarian Algorithm where the objective cost is determined from a Kalman Filter corrected Kanade-Lucas-Tomasi (KLT) Tracker. In order to correct the estimated count from tracking process, we combine tracking results with a Structure from Motion (SfM) algorithm to calculate relative 3D locations and size estimates to reject outliers and double counted fruit tracks. We evaluate our algorithm by comparing with ground-truth human-annotated visual counts. Our results demonstrate that our pipeline is able to accurately and reliably count fruits across image sequences, and the correction step can signiﬁcantly improve the counting accuracy and robustness. Although discussed in the context of fruit counting, our work can extend to detection, tracking, and counting of a variety of other stationary features of interest such as leaf-spots, wilt, and blossom.},
	language = {en},
	urldate = {2018-07-28},
	journal = {arXiv:1804.00307 [cs]},
	author = {Liu, Xu and Chen, Steven W. and Aditya, Shreyas and Sivakumar, Nivedha and Dcunha, Sandeep and Qu, Chao and Taylor, Camillo J. and Das, Jnaneshwar and Kumar, Vijay},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.00307},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 8 pages, Submitted to IROS 2018 (2018 IEEE/RSJ International Conference on Intelligent Robots and Systems)},
	file = {Liu et al. - 2018 - Robust Fruit Counting Combining Deep Learning, Tr.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/CLJRIAJM/Liu et al. - 2018 - Robust Fruit Counting Combining Deep Learning, Tr.pdf:application/pdf}
}

@inproceedings{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298965/},
	doi = {10.1109/CVPR.2015.7298965},
	urldate = {2018-07-28},
	publisher = {IEEE},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = jun,
	year = {2015},
	pages = {3431--3440}
}

@inproceedings{lucas_iterative_1981,
	title = {An {Iterative} {Image} {Registration} {Technique} with an {Application} to {Stereo} {Vision}},
	booktitle = {{IJCAI}},
	author = {Lucas, Bruce D. and Kanade, Takeo},
	year = {1981}
}

@article{kuhn_hungarian_1955,
	title = {The {Hungarian} method for the assignment problem},
	volume = {2},
	issn = {00281441, 19319193},
	url = {http://doi.wiley.com/10.1002/nav.3800020109},
	doi = {10.1002/nav.3800020109},
	language = {en},
	number = {1-2},
	urldate = {2018-07-28},
	journal = {Naval Research Logistics Quarterly},
	author = {Kuhn, H. W.},
	month = mar,
	year = {1955},
	pages = {83--97}
}

@article{beucher_watershed_1992,
	title = {{THE} {WATERSHED} {TRANSFORMATION} {APPLIED} {TO} {IMAGE} {SEGMENTATION}},
	abstract = {Image segmentation by mathematical morphology is a methodology based upon the notions of watershed and homotopy modification. This paper aims at introducing this methodology through various examples of segmentation in materials sciences, electron microscopy and scene analysis.},
	language = {en},
	journal = {SCANNING MICROSCOPY-SUPPLEMENT},
	author = {Beucher, S},
	year = {1992},
	pages = {26},
	file = {BEUCHER - THE WATERSHED TRANSFORMATION APPLIED TO IMAGE SEGM.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/93UQKAF3/BEUCHER - THE WATERSHED TRANSFORMATION APPLIED TO IMAGE SEGM.pdf:application/pdf}
}

@article{pedersen_circular_2007,
	title = {Circular {Hough} {Transform}},
	volume = {123},
	language = {en},
	journal = {Aalborg University, Vision, Graphics, and Interactive Systems},
	author = {Pedersen, Simon Just Kjeldgaard},
	year = {2007},
	pages = {6},
	file = {Pedersen - Aalborg University, Vision, Graphics, and Interact.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/K7Z8PTM8/Pedersen - Aalborg University, Vision, Graphics, and Interact.pdf:application/pdf}
}

@inproceedings{hani_apple_2018,
	title = {Apple {Counting} using {Convolutional} {Neural} {Networks}},
	booktitle = {Intelligent {Robots} and {Systems} ({IROS}), 2018 {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {H{\"{a}}ni, Nicolai and Roy, Pravakar and Isler Volkan},
	year = {2018}
}

@article{baker_lucas-kanade_2004,
	title = {Lucas-{Kanade} 20 {Years} {On}: {A} {Unifying} {Framework}},
	volume = {56},
	issn = {0920-5691},
	shorttitle = {Lucas-{Kanade} 20 {Years} {On}},
	url = {http://link.springer.com/10.1023/B:VISI.0000011205.11775.fd},
	doi = {10.1023/B:VISI.0000011205.11775.fd},
	abstract = {Since the Lucas-Kanade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. Applications range from optical ﬂow and tracking to layered motion, mosaic construction, and face coding. Numerous algorithms have been proposed and a wide variety of extensions have been made to the original formulation. We present an overview of image alignment, describing most of the algorithms and their extensions in a consistent framework. We concentrate on the inverse compositional algorithm, an efﬁcient algorithm that we recently proposed. We examine which of the extensions to Lucas-Kanade can be used with the inverse compositional algorithm without any signiﬁcant loss of efﬁciency, and which cannot. In this paper, Part 1 in a series of papers, we cover the quantity approximated, the warp update rule, and the gradient descent approximation. In future papers, we will cover the choice of the error function, how to allow linear appearance variation, and how to impose priors on the parameters.},
	language = {en},
	number = {3},
	urldate = {2018-07-31},
	journal = {International Journal of Computer Vision},
	author = {Baker, Simon and Matthews, Iain},
	month = feb,
	year = {2004},
	pages = {221--255},
	file = {Baker and Matthews - 2004 - Lucas-Kanade 20 Years On A Unifying Framework.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/2LESIMUP/Baker and Matthews - 2004 - Lucas-Kanade 20 Years On A Unifying Framework.pdf:application/pdf}
}

@incollection{hutchison_multi-stage_2012,
	address = {Berlin, Heidelberg},
	title = {A {Multi}-stage {Linear} {Approach} to {Structure} from {Motion}},
	volume = {6554},
	isbn = {978-3-642-35739-8 978-3-642-35740-4},
	url = {http://link.springer.com/10.1007/978-3-642-35740-4_21},
	abstract = {We present a new structure from motion (Sfm) technique based on point and vanishing point (VP) matches in images. First, all global camera rotations are computed from VP matches as well as relative rotation estimates obtained from pairwise image matches. A new multi-staged linear technique is then used to estimate all camera translations and 3D points simultaneously. The proposed method involves ﬁrst performing pairwise reconstructions, then robustly aligning these in pairs, and ﬁnally aligning all of them globally by simultaneously estimating their unknown relative scales and translations. In doing so, measurements inconsistent in three views are eﬃciently removed. Unlike sequential Sfm, the proposed method treats all images equally, is easy to parallelize and does not require intermediate bundle adjustments. There is also a reduction of drift and signiﬁcant speedups up to two order of magnitude over sequential Sfm. We compare our method with a standard Sfm pipeline [1] and demonstrate that our linear estimates are accurate on a variety of datasets, and can serve as good initializations for ﬁnal bundle adjustment. Because we exploit VPs when available, our approach is particularly well-suited to the reconstruction of man-made scenes.},
	language = {en},
	urldate = {2018-07-31},
	booktitle = {Trends and {Topics} in {Computer} {Vision}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sinha, Sudipta N. and Steedly, Drew and Szeliski, Richard},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Kutulakos, Kiriakos N.},
	year = {2012},
	doi = {10.1007/978-3-642-35740-4_21},
	pages = {267--281},
	file = {Sinha et al. - 2012 - A Multi-stage Linear Approach to Structure from Mo.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/TJQUF2IK/Sinha et al. - 2012 - A Multi-stage Linear Approach to Structure from Mo.pdf:application/pdf}
}

@article{badrinarayanan_segnet:_2015,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	language = {en},
	urldate = {2018-08-09},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/G8A8LFHJ/Badrinarayanan et al. - 2015 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf}
}



@misc{dutta_vgg_2016,
	title = {{VGG} {Image} {Annotator} ({VIA})},
	url = {http://www.robots.ox.ac.uk/~vgg/software/via/},
	urldate = {2018-08-10},
	author = {Dutta, A and Gupta, A and Zissermann, A},
	year = {2016}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2018-08-12},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/ZB2M7GAW/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the ﬁeld of large-scale image classiﬁcation and object detection, and compare the state-ofthe-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2018-08-12},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	pages = {211--252},
	file = {Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/N6NCVN65/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2018-08-12},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/4YPGZTZM/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@article{szegedy_inception-v4_2016,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	url = {http://arxiv.org/abs/1602.07261},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any beneﬁt in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks signiﬁcantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classiﬁcation task signiﬁcantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08\% top-5 error on the test set of the ImageNet classiﬁcation (CLS) challenge.},
	language = {en},
	urldate = {2018-08-12},
	journal = {arXiv:1602.07261 [cs]},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.07261},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of R.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/DZ4YSFM8/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of R.pdf:application/pdf}
}

@article{abadi_tensorflow:_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	language = {en},
	author = {Abadi, Martın and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2015},
	pages = {19},
	file = {Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/7ZRYLWQV/Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf}
}

@misc{chollet_keras_2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, Francoise},
	year = {2015}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2018-08-12},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/WHSDDKIC/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{huang_speed/accuracy_2016,
	title = {Speed/accuracy trade-offs for modern convolutional object detectors},
	url = {http://arxiv.org/abs/1611.10012},
	abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difﬁcult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a uniﬁed implementation of the Faster R-CNN [31], R-FCN [6] and SSD [26] systems, which we view as “meta-architectures” and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
	language = {en},
	urldate = {2018-08-12},
	journal = {arXiv:1611.10012 [cs]},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.10012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2017},
	file = {Huang et al. - 2016 - Speedaccuracy trade-offs for modern convolutional.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/VYW6GNKX/Huang et al. - 2016 - Speedaccuracy trade-offs for modern convolutional.pdf:application/pdf}
}

@article{liu_ssd:_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	language = {en},
	urldate = {2018-08-12},
	journal = {arXiv:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv: 1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
	annote = {Comment: ECCV 2016},
	file = {Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/UDT2PBEA/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf}
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the ImageNet classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2018-08-12},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/S3VU2FLF/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf}
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2018-08-12},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/J3EPMKMZ/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf}
}

@article{achanta2012slic,
  title={SLIC superpixels compared to state-of-the-art superpixel methods},
  author={Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={11},
  pages={2274--2282},
  year={2012},
  publisher={IEEE}
}
@article{roy2018arxiv,
  title={Vision-based preharvest yield mapping for apple orchards},
  author={Roy, Pravakar and Kislay, Abhijeet and Plonski, Patrick A and Luby, James and Isler, Volkan},
  journal={Computers and Electronics in Agriculture},
  volume={164},
  pages={104897},
  year={2019},
  publisher={Elsevier}
}


@inproceedings{goldbergerKLdivergence,
  title={An Efficient Image Similarity Measure Based on Approximations of KL-Divergence Between Two Gaussian Mixtures.},
  author={Goldberger, Jacob and Gordon, Shiri and Greenspan, Hayit and others},
  booktitle={ICCV},
  volume={3},
  pages={487--493},
  year={2003}
}

@article{chen2017counting,
  title={Counting Apples and Oranges With Deep Learning: A Data-Driven Approach},
  author={Chen, Steven W and Shivakumar, Shreyas S and Dcunha, Sandeep and Das, Jnaneshwar and Okon, Edidiong and Qu, Chao and Taylor, Camillo J and Kumar, Vijay},
  journal={IEEE Robotics and Automation Letters},
  volume={2},
  number={2},
  pages={781--788},
  year={2017},
  publisher={IEEE}
}


@article{lin_feature_2016,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiﬁcant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	language = {en},
	urldate = {2018-10-17},
	journal = {arXiv:1612.03144 [cs]},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.03144},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lin et al. - 2016 - Feature Pyramid Networks for Object Detection.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/4AKJQ44M/Lin et al. - 2016 - Feature Pyramid Networks for Object Detection.pdf:application/pdf}
}

@article{lin_focal_2017,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	language = {en},
	urldate = {2018-10-17},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/5XEEK89T/Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf:application/pdf}
}

@article{dong2018semantic,
  title={Semantic mapping for orchard environments by merging two-sides reconstructions of tree rows},
  author={Dong, Wenbo and Roy, Pravakar and Isler, Volkan},
  journal={Journal of Field Robotics},
  year={2019},
  publisher={Wiley Online Library}
}

@article{zeiler2012adadelta,
  title={ADADELTA: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{pedersen2007circular,
  title={Circular hough transform},
  author={Pedersen, Simon Just Kjeldgaard},
  journal={Aalborg University, Vision, Graphics, and Interactive Systems},
  volume={123},
  pages={123},
  year={2007}
}

@article{em,
  title={The expectation-maximization algorithm},
  author={Moon, Todd K},
  journal={IEEE Signal processing magazine},
  volume={13},
  number={6},
  pages={47--60},
  year={1996},
  publisher={IEEE}
}


@incollection{andreescu2004inclusion,
  title={Inclusion-Exclusion Principle},
  author={Andreescu, Titu and Feng, Zuming},
  booktitle={A Path to Combinatorics for Undergraduates},
  pages={117--141},
  year={2004},
  publisher={Springer}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@inproceedings{chuang2001bayesian,
  title={A bayesian approach to digital matting},
  author={Chuang, Yung-Yu and Curless, Brian and Salesin, David H and Szeliski, Richard},
  booktitle={Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on},
  volume={2},
  pages={II--II},
  year={2001},
  organization={IEEE}
}

@inproceedings{ruzon2000alpha,
  title={Alpha estimation in natural images},
  author={Ruzon, Mark A and Tomasi, Carlo},
  booktitle={Computer Vision and Pattern Recognition, 2000. Proceedings. IEEE Conference on},
  volume={1},
  pages={18--25},
  year={2000},
  organization={IEEE}
}

@article{bargoti2017image,
  title={Image segmentation for fruit detection and yield estimation in apple orchards},
  author={Bargoti, Suchet and Underwood, James P},
  journal={Journal of Field Robotics},
  year={2017},
  publisher={Wiley Online Library}
}

@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3431--3440},
  year={2015}
}


@article{gSLICr_2015,
    author = {{Ren}, C.~Y and {Prisacariu}, V.~A. and {Reid}, I.~D},
    title = "{gSLICr: SLIC superpixels at over 250Hz}",
    journal = {ArXiv e-prints},
    eprint = {1509.04232},
    year = 2015,
    month = sep
}



@inproceedings{gauch1992comparison,
  title={Comparison of three-color image segmentation algorithms in four color spaces},
  author={Gauch, John M and Hsia, Chi W},
  booktitle={Applications in optical science and engineering},
  pages={1168--1181},
  year={1992},
  organization={International Society for Optics and Photonics}
}

@article{faugeras1990motion,
  title={Motion from point matches: multiplicity of solutions},
  author={Faugeras, Olivier D and Maybank, Steve},
  journal={International Journal of Computer Vision},
  volume={4},
  number={3},
  pages={225--246},
  year={1990},
  publisher={Springer}
}

@inproceedings{wijewickrema2006reconstruction,
  author={Wijewickrema, Sudanthi NR and Paplinski, Andrew P and Esson, Charles E},
  Title={Reconstruction of spheres using occluding contours from stereo images},  
  booktitle={Pattern Recognition, 2006. ICPR 2006. 18th International Conference on},
  volume={1},
  pages={151--154},
  year={2006},
  organization={IEEE}
}


@INPROCEEDINGS{isfm, 
author={A. L. Rodríguez and P. E. López-de-Teruel and A. Ruiz}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on}, 
title={Reduced epipolar cost for accelerated incremental SfM}, 
year={2011}, 
pages={3097-3104}, 
ISSN={1063-6919}, 
month={June},}


@ONLINE{fortune,
author = {Clay Dillow},
title = {Despite {FAA} dithering, a drone economy sprouts on the farm},
year = {2014 (accessed Jul 2015)},
url = {http://fortune.com/2014/09/16/despite-faa-dithering-a-drone-economy-sprouts-on-the-farm/}
}

@online{diydrones,
	title={DIYDrones},
	author ={},
	year={2015 (accessed Jul 2015)},
	url={http://diydrones.com}
}

@online{pbs,
	title={Farms of the Future Will Run on Robots and Drones},
    author={Taylor Dobbs},
	year={2014 (accessed Jul 2015)},
	url={http://www.pbs.org/wgbh/nova/next/tech/farming-with-robotics-automation-and-sensors/}
}

@article{mulla2013twenty,
  title={Twenty five years of remote sensing in precision agriculture: Key advances and remaining knowledge gaps},
  author={Mulla, David J},
  journal={Biosystems Engineering},
  volume={114},
  number={4},
  pages={358--371},
  year={2013},
  publisher={Elsevier}
}


@article{dji,
	title={DJI F550},
	author ={},
	year={2015 (accessed Jul 2015)},
	url={http://www.dji.com}
}

@article{pixhawk,
	title={Pixhawk},
	author ={},
	year={2015 (accessed Jul 2015)},
	url={http://www.pixhawk.org}
}

@article{ardupilot,
	title={Ardupilot APM:Copter},
	year={2015 (accessed Jul 2015)},
	url={http://copter.ardupilot.com}
}


@article{odroid,
	title={ODROID-U3},
	author ={},
	year={2015 (accessed Jul 2015)},
	url={http://hardkernel.com}
}

@article{mavros,
	title={MAVROS},
	author ={},
	year={2015 (accessed Jul 2015)},
	url={http://wiki.ros.org/mavros}
}


@article{pointgrey,
    author = {},
	title={{Point Grey Chameleon USB 2.0 Cameras}},
	year={2015 (accessed Jul 2015)},
	url={http://ww2.ptgrey.com/USB2/chameleon}
}


@article{cloudcompare,
author = {},
version = {2.0},
title = {CloudCompare (version 2.0) [GPL software] (2016)},
url = {http://www.cloudcompare.org/}
}

@inproceedings{tokekar2013sensor,
  title={Sensor Planning for a Symbiotic UAV and UGV system for Precision Agriculture},
  author={Tokekar, Pratap and {Vander Hook}, Joshua and Mulla, David and Isler, Volkan},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5321--5326},
  year={2013},
  organization={IEEE}
}

@article{nuske2014automated,
  title={Automated Visual Yield Estimation in Vineyards},
  author={Nuske, Stephen and Wilshusen, Kyle and Achar, Supreeth and Yoder, Luke and Singh, Sanjiv},
  journal={Journal of Field Robotics},
  volume={31},
  number={5},
  pages={837--860},
  year={2014},
  publisher={Wiley Online Library}
}

@article{Wijewickrema2006,
author = {Sudanthi N.R. Wijewickrema, Andrew P. Paplinki and Charles E. Esson},
journal = {International Conference on Pattern Recognition},
number = {3},
pages = {18--21},	
title = {Reconstruction of spheres using occluding contours from stereo images},
volume = {0},
year = {2006}
}

@inproceedings{zhang20133d,
  title={3D perception for accurate row following: Methodology and results},
  author={Zhang, Ji and Chambers, Andrew and Maeta, Silvio and Bergerman, Marcel and Singh, Sanjiv},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5306--5313},
  year={2013},
  organization={IEEE}
}

@inproceedings{jagbrant2015lidar,
  title={LiDAR Based Tree and Platform Localisation in Almond Orchards},
  author={Jagbrant, Gustav and Underwood, James Patrick and Nieto, Juan and Sukkarieh, Salah},
  booktitle={Field and Service Robotics},
  pages={469--483},
  year={2015},
  organization={Springer}
}

@article{ball2013robotics,
  title={Robotics for Sustainable Broad-Acre Agriculture},
  author={Ball, David and Ross, Patrick and English, Andrew and Patten, Tim and Upcroft, Ben and Fitch, Robert and Sukkarieh, Salah and Wyeth, Gordon and Corke, Peter},
  journal={Proceedings of Field and Service Robotics (FSR 2013)},
  pages={1--14},
  year={2013},
  publisher={Springer-Verlag}
}

@inproceedings{bergerman2012results,
  title={Results with autonomous vehicles operating in specialty crops},
  author={Bergerman, Marcel and Singh, Sanjiv and Hamner, Bradley},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1829--1835},
  year={2012},
  organization={IEEE}
}

@article{bergerman2013ieee,
  title={IEEE robotics and automation society technical committee on agricultural robotics and automation},
  author={Bergerman, Marcel and Van Henten, Eldert and Billingsley, John and Reid, John and Mingcong, Deng},
  journal={IEEE Robotics and Automation Magazine},
  volume={20},
  number={2},
  pages={20--23},
  year={2013},
  publisher={IEEE}
}

@inproceedings{hung2013orchard,
  title={Orchard fruit segmentation using multi-spectral feature learning},
  author={Hung, Calvin and Nieto, Juan and Taylor, Zachary and Underwood, James and Sukkarieh, Salah},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5314--5320},
  year={2013},
  organization={IEEE}
}

@incollection{won2013view,
  title={View Planning of a Multi-rotor Unmanned Air Vehicle for Tree Modeling Using Silhouette-Based Shape Estimation},
  author={Won, Dae-Yeon and G{\"o}kto{\u{g}}an, Ali Haydar and Sukkarieh, Salah and Tahk, Min-Jea},
  booktitle={Frontiers of Intelligent Autonomous Systems},
  pages={193--207},
  year={2013},
  publisher={Springer}
}

@article{bajcsy1988active,
  title={Active perception},
  author={Bajcsy, Ruzena},
  journal={Proceedings of the IEEE},
  volume={76},
  number={8},
  pages={966--1005},
  year={1988},
  publisher={IEEE}
}



@article{hung2012multi,
  title={Multi-class predictive template for tree crown detection},
  author={Hung, Calvin and Bryson, Mitch and Sukkarieh, Salah},
  journal={ISPRS Journal of Photogrammetry and Remote Sensing},
  volume={68},
  pages={170--183},
  year={2012},
  publisher={Elsevier}
}


@inproceedings{english2014vision,
  title={Vision based guidance for robot navigation in agriculture},
  author={English, Andrew and Ross, Patrick and Ball, David and Corke, Peter},
  year={2014},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  organization={IEEE}
}


@ARTICLE{canny,
    author = {John Canny},
    title = {A computational approach to edge detection},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    year = {1986}
}
@book{opencv,
  title={Learning OpenCV: Computer vision with the OpenCV library},
  author={Bradski, Gary and Kaehler, Adrian},
  year={2008},
  publisher={" O'Reilly Media, Inc."}
}

@incollection{wang,
	year={2013},
	isbn={978-3-319-00064-0},
	booktitle={Experimental Robotics},
	volume={88},
	series={Springer Tracts in Advanced Robotics},
	editor={Desai, Jaydev P. and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},	
	title={Automated Crop Yield Estimation for Apple Orchards},	
	publisher={Springer International Publishing},
	author={Wang, Qi and Nuske, Stephen and Bergerman, Marcel and Singh, Sanjiv},
	pages={745-758},
	language={English}
}





@article{Linker,
 author = {Linker, Raphael and Cohen, Oded and Naor, Amos},
 title = {Determination of the Number of Green Apples in RGB Images Recorded in Orchards},
 journal = {Comput. Electron. Agric.},
 issue_date = {February, 2012},
 volume = {81},
 month = feb,
 year = {2012},
 issn = {0168-1699},
 pages = {45--57},
 numpages = {13}, 
 acmid = {2109303},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Computer vision, Fruit recognition, Image processing}
} 


@article{jimenez2000survey,
  title={A survey of computer vision methods for locating fruit on trees},
  author={Jimenez, AR and Ceres, R and Pons, JL},
  journal={Transactions of the ASAE-American Society of Agricultural Engineers},
  volume={43},
  number={6},
  pages={1911--1920},
  year={2000},
  publisher={[St. Joseph, Mich., American Society of Agricultural Engineers], 1958-c2005.}
}


@incollection{mao,
year={2009},
isbn={978-1-4419-0210-8},
booktitle={Computer and Computing Technologies in Agriculture II, Volume 2},
volume={294},
series={IFIP Advances in Information and Communication Technology},
editor={Li, Daoliang and Zhao, Chunjiang},
title={DETECTION AND POSITION METHOD OF APPLE TREE IMAGE},
publisher={Springer US},
author={Mao, Wenhua and Jia, Baoping and Zhang, Xiaochao and Hub, Xiaoan},
pages={1039-1048},
language={English}
}


@article{regsurvey,
title = "Image registration methods: a survey ",
journal = "Image and Vision Computing ",
volume = "21",
number = "11",
pages = "977 - 1000",
year = "2003",
note = "",
issn = "0262-8856",
author = "Barbara Zitová and Jan Flusser",
keywords = "Image registration",
keywords = "Feature detection",
keywords = "Feature matching",
keywords = "Mapping function",
keywords = "Resampling "
}

@ARTICLE{pointmatch, 
author={Besl, P.J. and McKay, Neil D.}, 
journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on}, 
title={A method for registration of 3-D shapes}, 
year={1992}, 
month={Feb}, 
volume={14}, 
number={2}, 
pages={239-256}, 
keywords={computational geometry;convergence of numerical methods;iterative methods;optimisation;pattern recognition;picture processing;3D shape registration;convergence;geometric entity;geometric model;iterative closest point;mean-square distance metric;pattern recognition;point set registration;Convergence;Inspection;Iterative algorithms;Iterative closest point algorithm;Iterative methods;Motion estimation;Quaternions;Shape measurement;Solid modeling;Testing}, 
ISSN={0162-8828}
}



@incollection{fastfeature,
  title={Machine learning for high-speed corner detection},
  author={Rosten, Edward and Drummond, Tom},
  booktitle={Computer Vision--ECCV 2006},
  pages={430--443},
  year={2006},
  publisher={Springer}
}


@incollection{surffeature,
  title={Surf: Speeded up robust features},
  author={Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
  booktitle={Computer vision--ECCV 2006},
  pages={404--417},
  year={2006},
  publisher={Springer}
}

@article{humoments,
  title={Visual pattern recognition by moment invariants},
  author={Hu, Ming-Kuei},
  journal={Information Theory, IRE Transactions on},
  volume={8},
  number={2},
  pages={179--187},
  year={1962},
  publisher={IEEE}
}


@article{3dalign,
  title={Estimating 3-D rigid body transformations: a comparison of four major algorithms},
  author={Eggert, David W and Lorusso, Adele and Fisher, Robert B},
  journal={Machine Vision and Applications},
  volume={9},
  number={5-6},
  pages={272--290},
  year={1997},
  publisher={Springer}
}


@article{sfm,
  title={Motion and structure from feature correspondences: A review},
  author={Huang, Thomas S and Netravali, Arun N},
  journal={Proceedings of the IEEE},
  volume={82},
  number={2},
  pages={252--268},
  year={1994},
  publisher={IEEE}
}

@article{visualsfm,
  title={VisualSFM: A visual structure from motion system},
  author={Wu, Changchang},
  journal={VisualSFM: A Visual Structure from Motion System},
  year={2011}
}


@inproceedings{sift,
  title={Object recognition from local scale-invariant features},
  author={Lowe, David G},
  booktitle={Computer vision, 1999. The proceedings of the seventh IEEE international conference on},
  volume={2},
  pages={1150--1157},
  year={1999},
  organization={Ieee}
}


@inproceedings{crosscorrelation,
  title={Template matching using fast normalized cross correlation},
  author={Briechle, Kai and Hanebeck, Uwe D},
  booktitle={Aerospace/Defense Sensing, Simulation, and Controls},
  pages={95--102},
  year={2001},
  organization={International Society for Optics and Photonics}
}

@incollection{bunad,
  title={Bundle adjustment in the large},
  author={Agarwal, Sameer and Snavely, Noah and Seitz, Steven M and Szeliski, Richard},
  booktitle={Computer Vision--ECCV 2010},
  pages={29--42},
  year={2010},
  publisher={Springer}
}


@article{Kannade,
  title={Lucas-kanade 20 years on: A unifying framework},
  author={Baker, Simon and Matthews, Iain},
  journal={International journal of computer vision},
  volume={56},
  number={3},
  pages={221--255},
  year={2004},
  publisher={Springer}
}

@inproceedings{ das2015devices,
    title = "Devices, Systems, and Methods for Automated Monitoring enabling Precision Agriculture",
    author = {Jnaneshwar Das and Gareth Cross and Chao Qu and Anurag Makineni and Pratap Tokekar and Yash Mulgaonkar and Vijay Kumar},
    booktitle = {Proceedings of IEEE Conference on Automation Science and Engineering},
    year = "2015",
}


@article{quartile,
  title={A survey of outlier detection methodologies},
  author={Hodge, Victoria J and Austin, Jim},
  journal={Artificial Intelligence Review},
  volume={22},
  number={2},
  pages={85--126},
  year={2004},
  publisher={Springer}
}

@article{camcalib,
  title={A flexible new technique for camera calibration},
  author={Zhang, Zhengyou},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={22},
  number={11},
  pages={1330--1334},
  year={2000},
  publisher={IEEE}
}



@techreport{roy2015tr,
     title = {Robotic Surveying of Apple Orchards},
     author = {Pravakar Roy and Nikolaos Stefas and Cheng Peng and Haluk Bayram and Pratap Tokekar and Volkan Isler},
     group = {RSN},
     year = {2015},
     institution = {University of Minnesota, Department of Computer Science},
     month = {07},
     Date-Added = {2015-07-18},
    
}

@article{pnp,
  title={Linear n-point camera pose determination},
  author={Quan, Long and Lan, Zhongdan},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={21},
  number={8},
  pages={774--780},
  year={1999},
  publisher={IEEE}
}

@book{hartley2003multiple,
  title={Multiple view geometry in computer vision},
  author={Hartley, Richard and Zisserman, Andrew},
  year={2003},
  publisher={Cambridge university press}
}


@article{nister,
  title={An efficient solution to the five-point relative pose problem},
  author={Nist{\'e}r, David},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={26},
  number={6},
  pages={756--770},
  year={2004},
  publisher={IEEE}
}


@article{scaramuzza2011visual,
  title={Visual odometry [tutorial]},
  author={Scaramuzza, Davide and Fraundorfer, Friedrich},
  journal={Robotics \& Automation Magazine, IEEE},
  volume={18},
  number={4},
  pages={80--92},
  year={2011},
  publisher={IEEE}
}


@misc{sinha2014multi,
  title={Multi-stage linear structure from motion},
  author={Sinha, Sudipta Narayan and Steedly, Drew Edward and Szeliski, Richard S},
  year={2014},
  month=sep # "~16",
  publisher={Google Patents},
  note={US Patent 8,837,811}
}


@inproceedings{torr1997assessment,
  title={An assessment of information criteria for motion model selection},
  author={Torr, Philip HS},
  booktitle={Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on},
  pages={47--52},
  year={1997},
  organization={IEEE}
}


@inproceedings{parsonage2011efficient,
  title={Efficient dense reconstruction from video},
  author={Parsonage, Phil and Hilton, Adrian and Starck, Jon},
  booktitle={Visual Media Production (CVMP), 2011 Conference for},
  pages={30--38},
  year={2011},
  organization={IEEE}
}


@inproceedings{hung2015feature,
  title={A feature learning based approach for automated fruit yield estimation},
  author={Hung, Calvin and Underwood, James and Nieto, Juan and Sukkarieh, Salah},
  booktitle={Field and Service Robotics},
  pages={485--498},
  year={2015},
  organization={Springer}
}

@article{rubner2000earth,
  title={The earth mover's distance as a metric for image retrieval},
  author={Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J},
  journal={International journal of computer vision},
  volume={40},
  number={2},
  pages={99--121},
  year={2000},
  publisher={Springer}
}



@article{aic,
  title={A new look at the statistical model identification},
  author={Akaike, Hirotugu},
  journal={Automatic Control, IEEE Transactions on},
  volume={19},
  number={6},
  pages={716--723},
  year={1974},
  publisher={Ieee}
}

@article{mdl,
  title={A universal prior for integers and estimation by minimum description length},
  author={Rissanen, Jorma},
  journal={The Annals of statistics},
  pages={416--431},
  year={1983},
  publisher={JSTOR}
}
@inproceedings{bic,
  title={Clustering via the Bayesian information criterion with applications in speech recognition},
  author={Chen, Scott Shaobing and Gopalakrishnan, Ponani S},
  booktitle={Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on},
  volume={2},
  pages={645--648},
  year={1998},
  organization={IEEE}
}

@article{kmeans,
  title={Algorithm AS 136: A k-means clustering algorithm},
  author={Hartigan, John A and Wong, Manchek A},
  journal={Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume={28},
  number={1},
  pages={100--108},
  year={1979},
  publisher={JSTOR}
}


@article{matlab,
  title={MATLAB and Statistics Toolbox Release 2015b, },
  author={The MathWorks Inc},
  institution={The MathWorks Inc, Natick, Massachusetts, United States.},
  year={2015}
}

@article{mdt,
  title={A tutorial introduction to the minimum description length principle},
  author={Grunwald, Peter},
  journal={arXiv preprint math/0406077},
  year={2004}
}

@article{gongal2016apple,
  title={Apple crop-load estimation with over-the-row machine vision system},
  author={Gongal, A and Silwal, A and Amatya, S and Karkee, M and Zhang, Q and Lewis, K},
  journal={Computers and Electronics in Agriculture},
  volume={120},
  pages={26--35},
  year={2016},
  publisher={Elsevier}
}

@article{zhouGala,
  title={Using colour features of cv.`Gala' apple fruits in an orchard in image processing to predict yield},
  author={Zhou, Rong and Damerow, Lutz and Sun, Yurui and Blanke, Michael M},
  journal={Precision Agriculture},
  volume={13},
  number={5},
  pages={568--580},
  year={2012},
  publisher={Springer}
}

@article{changyi2015apple,
  title={Apple detection from apple tree image based on BP neural network and Hough transform},
  author={Changyi, Xiao and Lihua, Zheng and Minzan, Li and Yuan, Chen and Chunyan, Mai},
  journal={International Journal of Agricultural and Biological Engineering},
  volume={8},
  number={6},
  pages={46--53},
  year={2015}
}

@article{liu2016method,
  title={A method of segmenting apples at night based on color and position information},
  author={Liu, Xiaoyang and Zhao, Dean and Jia, Weikuan and Ruan, Chengzhi and Tang, Shuping and Shen, Tian},
  journal={Computers and Electronics in Agriculture},
  volume={122},
  pages={118--123},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{hecht1989theory,
  title={Theory of the backpropagation neural network},
  author={Hecht-Nielsen, Robert},
  booktitle={Neural Networks, 1989. IJCNN., International Joint Conference on},
  pages={593--605},
  year={1989},
  organization={IEEE}
}

@article{dudacht,
  title={Use of the Hough transformation to detect lines and curves in pictures},
  author={Duda, Richard O and Hart, Peter E},
  journal={Communications of the ACM},
  volume={15},
  number={1},
  pages={11--15},
  year={1972},
  publisher={ACM}
}


@inproceedings{tabb2006segmentation,
  title={Segmentation of apple fruit from video via background modeling},
  author={Tabb, Amy L and Peterson, Donald L and Park, Johnny},
  booktitle={2006 ASAE Annual Meeting},
  pages={1},
  year={2006},
  organization={American Society of Agricultural and Biological Engineers}
}

@article{silwal2014apple,
  title={Apple identification in field environment with over the row machine vision system},
  author={Silwal, Abhisesh and Gongal, Aleana and Karkee, Manoj},
  journal={Agricultural Engineering International: CIGR Journal},
  volume={16},
  number={4},
  pages={66--75},
  year={2014}
}

@inproceedings{lucas1981iterative,
  title={An iterative image registration technique with an application to stereo vision.},
  author={Lucas, Bruce D and Kanade, Takeo and others},
  booktitle={IJCAI},
  volume={81},
  pages={674--679},
  year={1981}
}

@inproceedings{ma2015real,
  title={Real-Time Affine Tracking Using Re-located Lucas-Kanade Algorithm},
  author={Ma, X and Fan, N and He, Z and Yang, W},
  booktitle={2015 Third International Conference on Robot, Vision and Signal Processing (RVSP)},
  pages={35--38},
  year={2015},
  organization={IEEE}
}

@inproceedings{kwon2009visual,
  title={Visual tracking via geometric particle filtering on the affine group with optimal importance functions},
  author={Kwon, Junghyun and Lee, Kyoung Mu and Park, Frank Chongwoo},
  booktitle={Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  pages={991--998},
  year={2009},
  organization={IEEE}
}

@article{snavely2008modeling,
  title={Modeling the world from internet photo collections},
  author={Snavely, Noah and Seitz, Steven M and Szeliski, Richard},
  journal={International Journal of Computer Vision},
  volume={80},
  number={2},
  pages={189--210},
  year={2008},
  publisher={Springer}
}

@inproceedings{agarwal2009building,
  title={Building rome in a day},
  author={Agarwal, Sameer and Snavely, Noah and Simon, Ian and Seitz, Steven M and Szeliski, Richard},
  booktitle={Computer Vision, 2009 IEEE 12th International Conference on},
  pages={72--79},
  year={2009},
  organization={IEEE}
}
@inproceedings{goesele2007multi,
  title={Multi-view stereo for community photo collections},
  author={Goesele, Michael and Snavely, Noah and Curless, Brian and Hoppe, Hugues and Seitz, Steven M},
  booktitle={Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on},
  pages={1--8},
  year={2007},
  organization={IEEE}
}

@article{quan1999linear,
  title={Linear n-point camera pose determination},
  author={Quan, Long and Lan, Zhongdan},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={21},
  number={8},
  pages={774--780},
  year={1999},
  publisher={IEEE}
}
@inproceedings{hesch2011direct,
  title={A direct least-squares (DLS) method for PnP},
  author={Hesch, Joel A and Roumeliotis, Stergios I},
  booktitle={Computer Vision (ICCV), 2011 IEEE International Conference on},
  pages={383--390},
  year={2011},
  organization={IEEE}
}

@article{lepetit2009epnp,
  title={Epnp: An accurate o (n) solution to the pnp problem},
  author={Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
  journal={International journal of computer vision},
  volume={81},
  number={2},
  pages={155--166},
  year={2009},
  publisher={Springer}
}

@article{senthilnath2016detection,
  title={Detection of tomatoes using spectral-spatial methods in remotely sensed RGB images captured by UAV},
  author={Senthilnath, J and Dokania, Akanksha and Kandukuri, Manasa and Ramesh, KN and Anand, Gautham and Omkar, SN},
  journal={Biosystems Engineering},
  year={2016},
  publisher={Elsevier}
}

@article{som,
  title={Self-organizing maps of symbol strings},
  author={Kohonen, Teuvo and Somervuo, Panu},
  journal={Neurocomputing},
  volume={21},
  number={1},
  pages={19--30},
  year={1998},
  publisher={Elsevier}
}

@article{cht,
  title={Circular hough transform},
  author={Pedersen, Simon Just Kjeldgaard},
  journal={Aalborg University, Vision, Graphics, and Interactive Systems},
  volume={123},
  year={2007}
}

@inproceedings{kmeans++,
  title={k-means++: The advantages of careful seeding},
  author={Arthur, David and Vassilvitskii, Sergei},
  booktitle={Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
  pages={1027--1035},
  year={2007},
  organization={Society for Industrial and Applied Mathematics}
}


@InProceedings{roy2016counting,
author="Roy, Pravakar
and Isler, Volkan",
editor="Kuli{\'{c}}, Dana
and Nakamura, Yoshihiko
and Khatib, Oussama
and Venture, Gentiane",
title="Vision-Based Apple Counting and Yield Estimation",
booktitle="2016 International Symposium on Experimental Robotics",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="478--487",
isbn="978-3-319-50115-4"
}


@article{peng2016semantic,
  title={Semantic Mapping of Orchards},
  author={Peng, Cheng and Roy, Pravakar and Luby, James and Isler, Volkan},
  journal={IFAC-PapersOnLine},
  volume={49},
  number={16},
  pages={85--89},
  year={2016},
  publisher={Elsevier}
}

@article{stefas2016vision,
  title={Vision-Based UAV Navigation in Orchards},
  author={Stefas, Nikolaos and Bayram, Haluk and Isler, Volkan},
  journal={IFAC-PapersOnLine},
  volume={49},
  number={16},
  pages={10--15},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{roy2016surveying,
  title={Surveying apple orchards with a monocular vision system},
  author={Roy, Pravakar and Isler, Volkan},
  booktitle={Automation Science and Engineering (CASE), 2016 IEEE International Conference on},
  pages={916--921},
  year={2016},
  organization={IEEE}
}


@misc{isler2016robotic,
  title={Robotic surveying of fruit plants},
  author={Isler, Volkan and Tokekar, Pratap Rajkumar and Stefas, Nikolaos and Roy, Pravakar},
  year={2016},
  month=apr # "~18",
  note={US Patent App. 15/131,745}
}

@inproceedings{eshel2008homography,
  title={Homography based multiple camera detection and tracking of people in a dense crowd},
  author={Eshel, Ran and Moses, Yael},
  booktitle={Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
  pages={1--8},
  year={2008},
  organization={IEEE}
}


@article{specialtyCrops,
  title={Advanced Engineering Systems for Specialty Crops: A Review of Precision Agriculture for Water},
  author={Downey, D and Ehsani, R and Giles, K and Haneklaus, S and Karimi, D and Panten, K and Pierce, F and Schnug, E and Slaughter, DC and Upadhyaya, S and others},
  journal={Chemical, and Nutrient Application, and Yield Monitoring: Landbauforschung--vTI Agriculture and Forestry Research, Special issue},
  number={340},
  pages={1--88},
  year={2010}
}

@article{deepApple,
  author    = {Suchet Bargoti and
               James Patrick Underwood},
  title     = {Image Segmentation for Fruit Detection and Yield Estimation in Apple
               Orchards},
  journal   = {CoRR},
  volume    = {abs/1610.08120},
  year      = {2016},
  timestamp = {Wed, 02 Nov 2016 09:51:26 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BargotiU16a},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@inproceedings{intersectionunion,
  title={Simultaneous detection and segmentation},
  author={Hariharan, Bharath and Arbel{\'a}ez, Pablo and Girshick, Ross and Malik, Jitendra},
  booktitle={European Conference on Computer Vision},
  pages={297--312},
  year={2014},
  organization={Springer}
}

@article{stein2016image,
  title={Image Based Mango Fruit Detection, Localisation and Yield Estimation Using Multiple View Geometry},
  author={Stein, Madeleine and Bargoti, Suchet and Underwood, James},
  journal={Sensors},
  volume={16},
  number={11},
  pages={1915},
  year={2016},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{myronenko2010point,
  title={Point set registration: Coherent point drift},
  author={Myronenko, Andriy and Song, Xubo},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={32},
  number={12},
  pages={2262--2275},
  year={2010},
  publisher={IEEE}
}

@article{edelsbrunner1994three,
  title={Three-dimensional alpha shapes},
  author={Edelsbrunner, Herbert and M{\"u}cke, Ernst P},
  journal={ACM Transactions on Graphics (TOG)},
  volume={13},
  number={1},
  pages={43--72},
  year={1994},
  publisher={ACM}
}

@misc{agisoft,
  author={LLC,Agisoft},
  title = {{Agisoft PhotoScan}},
  howpublished = {\url{http:http://www.agisoft.com/}},
  note = {Accessed: 2017-09-15},
  year={2017}
}

@incollection{more1978levenberg,
  title={The Levenberg-Marquardt algorithm: implementation and theory},
  author={Mor{\'e}, Jorge J},
  booktitle={Numerical analysis},
  pages={105--116},
  year={1978},
  publisher={Springer}
}

@inproceedings{yang2010plane,
  title={Plane detection in point cloud data},
  author={Yang, Michael Ying and F{\"o}rstner, Wolfgang},
  booktitle={Proceedings of the 2nd int conf on machine control guidance, Bonn},
  volume={1},
  pages={95--104},
  year={2010}
}
@incollection{andreescu2004inclusion,
  title={Inclusion-Exclusion Principle},
  author={Andreescu, Titu and Feng, Zuming},
  booktitle={A Path to Combinatorics for Undergraduates},
  pages={117--141},
  year={2004},
  publisher={Springer}
}

@article{fischler1981random,
  title={Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
  author={Fischler, Martin A and Bolles, Robert C},
  journal={Communications of the ACM},
  volume={24},
  number={6},
  pages={381--395},
  year={1981},
  publisher={ACM}
}

@article{beder2006direct,
  title={Direct solutions for computing cylinders from minimal sets of 3d points},
  author={Beder, Christian and F{\"o}rstner, Wolfgang},
  journal={Computer Vision--ECCV 2006},
  pages={135--146},
  year={2006},
  publisher={Springer}
}

@techreport{techreportroy,
  title={Registering Reconstructions of the Two Sides of Fruit Tree Rows },
  author={Roy, Pravakar and Dong, Wenbo and Isler, Volkan},
  year={2018},
  institution={Technical Report TR-18-008, University of Minnesota, Computer Science \& Engineering Department}
}

@techreport{dong2018treeSBA,
  title={Tree morphology for phenotyping from semantics-based mapping in orchard environments},
  author={Dong, Wenbo and Isler, Volkan},
  year={2018},
  institution={Technical Report TR-18-006, University of Minnesota, Computer Science \& Engineering Department}
}



@inproceedings{sturm2012benchmark,
  title={A benchmark for the evaluation of RGB-D SLAM systems},
  author={Sturm, J{\"u}rgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={573--580},
  year={2012},
  organization={IEEE}
}

@article{connolly1997study,
  title={A study of efficiency and accuracy in the transformation from RGB to CIELAB color space},
  author={Connolly, Christine and Fleiss, T},
  journal={IEEE Transactions on Image Processing},
  volume={6},
  number={7},
  pages={1046--1048},
  year={1997},
  publisher={IEEE}
}

@article{hani_jfr_counting,
   title     = {A Comparative Study of Fruit Detection and Counting Methods for Yield
               Mapping in Apple Orchards},
  author    = {Nicolai H{\"{a}}ni and
               Pravakar Roy and
               Volkan Isler},
  journal={Journal of Field Robotics},
  year={2019},
  publisher={Wiley Online Library}
}

@article{maver1993occlusions,
  title={Occlusions as a guide for planning the next view},
  author={Maver, Jasna and Bajcsy, Ruzena},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={15},
  number={5},
  pages={417--433},
  year={1993},
  publisher={IEEE}
}

@inproceedings{baur2014path,
  title={Path planning for a fruit picking manipulator},
  author={Baur, J{\"o}rg and Sch{\"u}tz, Christoph and Pfaff, Julian and Buschmann, Thomas and Ulbrich, Heinz},
  booktitle={Proceedings International Conference of Agricultural Engineering, Z{\"u}rich, Switzerland},
  year={2014}
}

@inproceedings{foix20153d,
  title={3D Sensor planning framework for leaf probing},
  author={Foix, Sergi and Alenya, Guillem and Torras, Carme},
  booktitle={Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on},
  pages={6501--6506},
  year={2015},
  organization={IEEE}
}

@article{scott2003view,
  title={View planning for automated three-dimensional object reconstruction and inspection},
  author={Scott, William R and Roth, Gerhard and Rivest, Jean-Fran{\c{c}}ois},
  journal={ACM Computing Surveys (CSUR)},
  volume={35},
  number={1},
  pages={64--96},
  year={2003},
  publisher={ACM}
}

@article{atanasov2014nonmyopic,
  title={Nonmyopic view planning for active object classification and pose estimation},
  author={Atanasov, Nikolay and Sankaran, Bharath and Le Ny, Jerome and Pappas, George J and Daniilidis, Kostas},
  journal={IEEE Transactions on Robotics},
  volume={30},
  number={5},
  pages={1078--1090},
  year={2014},
  publisher={IEEE}
}

@article{potthast2014probabilistic,
  title={A probabilistic framework for next best view estimation in a cluttered environment},
  author={Potthast, Christian and Sukhatme, Gaurav S},
  journal={Journal of Visual Communication and Image Representation},
  volume={25},
  number={1},
  pages={148--164},
  year={2014},
  publisher={Elsevier}
}




@article{bajcsy2016revisiting,
  title={Revisiting active perception},
  author={Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K},
  journal={arXiv preprint arXiv:1603.02729},
  year={2016}
}


@article{pattenmonte,
  title={Monte Carlo planning for active object classification},
  author={Patten, Timothy and Martens, Wolfram and Fitch, Robert},
  journal={Autonomous Robots},
  pages={1--31},
  publisher={Springer},
  year={2017}
}

@inproceedings{cross1998quadric,
  title={Quadric reconstruction from dual-space geometry},
  author={Cross, Geoffrey and Zisserman, Andrew},
  booktitle={Computer Vision, 1998. Sixth International Conference on},
  pages={25--31},
  year={1998},
  organization={IEEE}
}

@techreport{techreport,
     title = {Robotic Surveying of Apple Orchards},
     author = {Pravakar Roy and Nikolaos Stefas and Cheng Peng and Haluk Bayram and Pratap Tokekar and Volkan Isler},
     group = {RSN},
     year = {2015},
     institution = {University of Minnesota, Department of Computer Science},
     month = {07},
     Date-Added = {2015-07-18},
    
}

@incollection{andreescu_inclusion-exclusion_2004,
	title = {Inclusion-{Exclusion} {Principle}},
	booktitle = {A {Path} to {Combinatorics} for {Undergraduates}},
	publisher = {Springer},
	author = {Andreescu, Titu and Feng, Zuming},
	year = {2004},
	pages = {117--141}
}

@incollection{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680}
}

@inproceedings{zhu_unpaired_2017,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237506/},
	doi = {10.1109/ICCV.2017.244},
	language = {en},
	urldate = {2019-03-18},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = oct,
	year = {2017},
	pages = {2242--2251},
	file = {Zhu et al. - 2017 - Unpaired Image-to-Image Translation Using Cycle-Co.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/JZRAIJV5/Zhu et al. - 2017 - Unpaired Image-to-Image Translation Using Cycle-Co.pdf:application/pdf}
}

@inproceedings{roy2017active,
  title={Active view planning for counting apples in orchards},
  author={Roy, Pravakar and Isler, Volkan},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={6027--6032},
  year={2017},
  organization={IEEE}
}
@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	journal = {ICLR},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1409.1556},
	file = {Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/LZ2CXIIX/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf}
}

@article{badrinarayanan2017segnet,
  title={Segnet: A deep convolutional encoder-decoder architecture for image segmentation},
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={12},
  pages={2481--2495},
  year={2017},
  publisher={IEEE}
}


@incollection{leibe_perceptual_2016,
	address = {Cham},
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	volume = {9906},
	isbn = {978-3-319-46474-9 978-3-319-46475-6},
	url = {http://link.springer.com/10.1007/978-3-319-46475-6_43},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by deﬁning and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the beneﬁts of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	language = {en},
	urldate = {2018-11-13},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46475-6_43},
	pages = {694--711},
	file = {Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/8QHBVZTJ/Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf}
}

@article{ulyanov_instance_2016,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	shorttitle = {Instance {Normalization}},
	url = {http://arxiv.org/abs/1607.08022},
	abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et al. (2016). We show how a small change in the stylization architecture results in a signiﬁcant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code is available at https://github.com/DmitryUlyanov/texture\_nets. Full paper can be found at https://arxiv.org/abs/1701.02096.},
	language = {en},
	urldate = {2018-11-13},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.08022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ulyanov et al. - 2016 - Instance Normalization The Missing Ingredient for.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/5X2MQYJ3/Ulyanov et al. - 2016 - Instance Normalization The Missing Ingredient for.pdf:application/pdf}
}

@article{shrivastava_learning_2016,
	title = {Learning from {Simulated} and {Unsupervised} {Images} through {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1612.07828},
	abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator’s output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modiﬁcations to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a ‘self-regularization’ term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of reﬁned images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a signiﬁcant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
	language = {en},
	urldate = {2018-11-15},
	journal = {arXiv:1612.07828 [cs]},
	author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.07828},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted at CVPR 2017 for oral presentation},
	file = {Shrivastava et al. - 2016 - Learning from Simulated and Unsupervised Images th.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/MHSWHYVF/Shrivastava et al. - 2016 - Learning from Simulated and Unsupervised Images th.pdf:application/pdf}
}





@inproceedings{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	booktitle = {{NIPS}-{W}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017}
}

@article{taigman_unsupervised_2016,
	title = {Unsupervised {Cross}-{Domain} {Image} {Generation}},
	url = {http://arxiv.org/abs/1611.02200},
	abstract = {We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T , we would like to learn a generative function G that maps an input sample from S to the domain T , such that the output of a given function f , which accepts inputs in either domains, would remain unchanged. Other than the function f , the training data is unsupervised and consist of a set of samples from each domain.},
	language = {en},
	urldate = {2019-03-18},
	journal = {arXiv:1611.02200 [cs]},
	author = {Taigman, Yaniv and Polyak, Adam and Wolf, Lior},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02200},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Taigman et al. - 2016 - Unsupervised Cross-Domain Image Generation.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/X2QN8QIE/Taigman et al. - 2016 - Unsupervised Cross-Domain Image Generation.pdf:application/pdf}
}

@inproceedings{xian_texturegan:_2018,
	address = {Salt Lake City, UT},
	title = {{TextureGAN}: {Controlling} {Deep} {Image} {Synthesis} with {Texture} {Patches}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{TextureGAN}},
	url = {https://ieeexplore.ieee.org/document/8578980/},
	doi = {10.1109/CVPR.2018.00882},
	urldate = {2019-03-18},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Xian, Wenqi and Sangkloy, Patsorn and Agrawal, Varun and Raj, Amit and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
	month = jun,
	year = {2018},
	pages = {8456--8465}
}

@inproceedings{ledig_photo-realistic_2017,
	address = {Honolulu, HI},
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099502/},
	doi = {10.1109/CVPR.2017.19},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the ﬁner texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the ﬁdelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the ﬁrst framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely signiﬁcant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	language = {en},
	urldate = {2019-03-18},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	month = jul,
	year = {2017},
	pages = {105--114},
	file = {Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/Q3H3AZ8D/Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf}
}

@inproceedings{zhang_colorful_2016,
	title = {Colorful {Image} {Colorization}},
	booktitle = {{ECCV}},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A},
	year = {2016}
}

@inproceedings{eigen_predicting_2015,
	address = {Santiago, Chile},
	title = {Predicting {Depth}, {Surface} {Normals} and {Semantic} {Labels} with a {Common} {Multi}-scale {Convolutional} {Architecture}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410661/},
	doi = {10.1109/ICCV.2015.304},
	urldate = {2019-03-18},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Eigen, David and Fergus, Rob},
	month = dec,
	year = {2015},
	pages = {2650--2658}
}

@article{lu_sketch--image_2017,
	title = {Sketch-to-{Image} {Generation} {Using} {Deep} {Contextual} {Completion}},
	volume = {abs/1711.08972},
	url = {http://arxiv.org/abs/1711.08972},
	journal = {CoRR},
	author = {Lu, Yongyi and Wu, Shangzhe and Tai, Yu-Wing and Tang, Chi-Keung},
	year = {2017}
}

@article{isola_image--image_2016,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	volume = {abs/1611.07004},
	url = {http://arxiv.org/abs/1611.07004},
	journal = {CoRR},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	year = {2016}
}

@article{zhu_toward_2017,
	title = {Toward {Multimodal} {Image}-to-{Image} {Translation}},
	volume = {abs/1711.11586},
	url = {http://arxiv.org/abs/1711.11586},
	journal = {CoRR},
	author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A. and Wang, Oliver and Shechtman, Eli},
	year = {2017}
}

@article{wang_video--video_2018,
	title = {Video-to-{Video} {Synthesis}},
	volume = {abs/1808.06601},
	url = {http://arxiv.org/abs/1808.06601},
	journal = {CoRR},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Liu, Guilin and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	year = {2018}
}

@inproceedings{yi_dualgan:_2017,
	title = {{DualGAN}: {Unsupervised} {Dual} {Learning} for {Image}-{To}-{Image} {Translation}},
	booktitle = {The {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yi, Zili and Zhang, Hao and Tan, Ping and Gong, Minglun},
	month = oct,
	year = {2017}
}

@article{karacan_learning_2016,
	title = {Learning to {Generate} {Images} of {Outdoor} {Scenes} from {Attributes} and {Semantic} {Layouts}},
	volume = {abs/1612.00215},
	journal = {CoRR},
	author = {Karacan, Levent and Akata, Zeynep and Erdem, Aykut and Erdem, Erkut},
	year = {2016}
}

@inproceedings{wang_high-resolution_2018,
	title = {High-{Resolution} {Image} {Synthesis} and {Semantic} {Manipulation} with {Conditional} {GANs}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
	year = {2018}
}

@article{li_semantic-aware_2018,
	title = {Semantic-aware {Grad}-{GAN} for {Virtual}-to-{Real} {Urban} {Scene} {Adaption}},
	volume = {abs/1801.01726},
	url = {http://arxiv.org/abs/1801.01726},
	journal = {CoRR},
	author = {Li, Peilun and Liang, Xiaodan and Jia, Daoyuan and Xing, Eric P.},
	year = {2018}
}

@article{ramirez_exploiting_2018,
	title = {Exploiting {Semantics} in {Adversarial} {Training} for {Image}-{Level} {Domain} {Adaptation}},
	volume = {abs/1810.05852},
	url = {http://arxiv.org/abs/1810.05852},
	journal = {CoRR},
	author = {Ramirez, Pierluigi Zama and Tonioni, Alessio and Stefano, Luigi di},
	year = {2018}
}

@article{cherian_sem-gan:_2018,
	title = {Sem-{GAN}: {Semantically}-{Consistent} {Image}-to-{Image} {Translation}},
	volume = {abs/1807.04409},
	url = {http://arxiv.org/abs/1807.04409},
	journal = {CoRR},
	author = {Cherian, Anoop and Sullivan, Alan},
	year = {2018}
}

@article{liang_generative_2017,
	title = {Generative {Semantic} {Manipulation} with {Contrasting} {GAN}},
	volume = {abs/1708.00315},
	url = {http://arxiv.org/abs/1708.00315},
	journal = {CoRR},
	author = {Liang, Xiaodan and Zhang, Hao and Xing, Eric P.},
	year = {2017}
}

@incollection{alami_mejjati_unsupervised_2018,
	title = {Unsupervised {Attention}-guided {Image}-to-{Image} {Translation}},
	url = {http://papers.nips.cc/paper/7627-unsupervised-attention-guided-image-to-image-translation.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Alami Mejjati, Youssef and Richardt, Christian and Tompkin, James and Cosker, Darren and Kim, Kwang In},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {3693--3703}
}

@inproceedings{mo_instance-aware_2019,
	title = {Instance-aware {Image}-to-{Image} {Translation}},
	url = {https://openreview.net/forum?id=ryxwJhC9YX},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Mo, Sangwoo and Cho, Minsu and Shin, Jinwoo},
	year = {2019}
}

@article{tomei_art2real:_2018,
	title = {Art2Real: {Unfolding} the {Reality} of {Artworks} via {Semantically}-{Aware} {Image}-to-{Image} {Translation}},
	volume = {abs/1811.10666},
	journal = {CoRR},
	author = {Tomei, Matteo and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
	year = {2018}
}

@inproceedings{zhang_harmonic_2019,
	title = {Harmonic {Unpaired} {Image}-to-image {Translation}},
	url = {https://openreview.net/forum?id=S1M6Z2Cctm},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Rui and Pfister, Tomas and Li, Jia},
	year = {2019}
}

@inproceedings{benaim_one-sided_2017,
	title = {One-{Sided} {Unsupervised} {Domain} {Mapping}},
	booktitle = {{NIPS}},
	author = {Benaim, Sagie and Wolf, Lior},
	year = {2017}
}

@article{amodio_travelgan:_2019,
	title = {{TraVeLGAN}: {Image}-to-image {Translation} by {Transformation} {Vector} {Learning}},
	journal = {arXiv preprint arXiv:1902.09631},
	author = {Amodio, Matthew and Krishnaswamy, Smita},
	year = {2019}
}

@incollection{gonzalez-garcia_image--image_2018,
	title = {Image-to-image translation for cross-domain disentanglement},
	url = {http://papers.nips.cc/paper/7404-image-to-image-translation-for-cross-domain-disentanglement.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Gonzalez-Garcia, Abel and van de Weijer, Joost and Bengio, Yoshua},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1287--1298}
}

@article{hoffman_cycada:_2017,
	title = {Cycada: {Cycle}-consistent adversarial domain adaptation},
	journal = {arXiv preprint arXiv:1711.03213},
	author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei A and Darrell, Trevor},
	year = {2017}
}

@article{cordts_cityscapes_2016,
	title = {The {Cityscapes} {Dataset} for {Semantic} {Urban} {Scene} {Understanding}},
	volume = {abs/1604.01685},
	url = {http://arxiv.org/abs/1604.01685},
	journal = {CoRR},
	author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	year = {2016}
}

@inproceedings{hosseini-asl_augmented_2019,
	title = {Augmented {Cyclic} {Adversarial} {Learning} for {Low} {Resource} {Domain} {Adaptation}},
	url = {https://openreview.net/forum?id=B1G9doA9F7},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hosseini-Asl, Ehsan and Zhou, Yingbo and Xiong, Caiming and Socher, Richard},
	year = {2019}
}

@incollection{long_conditional_2018,
	title = {Conditional {Adversarial} {Domain} {Adaptation}},
	url = {http://papers.nips.cc/paper/7436-conditional-adversarial-domain-adaptation.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Long, Mingsheng and CAO, ZHANGJIE and Wang, Jianmin and Jordan, Michael I},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1640--1650}
}

@article{hassan_unsupervised_2018,
	title = {Unsupervised {Domain} {Adaptation} using {Generative} {Models} and {Self}-ensembling},
	volume = {abs/1812.00479},
	url = {http://arxiv.org/abs/1812.00479},
	journal = {CoRR},
	author = {Hassan, Eman T. and Chen, Xin and Crandall, David J.},
	year = {2018}
}

@article{shen_wasserstein_2017,
	title = {Wasserstein distance guided representation learning for domain adaptation},
	journal = {arXiv preprint arXiv:1707.01217},
	author = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
	year = {2017}
}

@inproceedings{zhang_collaborative_2018,
	address = {Salt Lake City, UT},
	title = {Collaborative and {Adversarial} {Network} for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578498/},
	doi = {10.1109/CVPR.2018.00400},
	abstract = {In this paper, we propose a new unsupervised domain adaptation approach called Collaborative and Adversarial Network (CAN) through domain-collaborative and domainadversarial training of neural networks. We add several domain classiﬁers on multiple CNN feature extraction blocks1, in which each domain classiﬁer is connected to the hidden representations from one block and one loss function is deﬁned based on the hidden presentation and the domain labels (e.g., source and target). We design a new loss function by integrating the losses from all blocks in order to learn domain informative representations from lower blocks through collaborative learning and learn domain uninformative representations from higher blocks through adversarial learning. We further extend our CAN method as Incremental CAN (iCAN), in which we iteratively select a set of pseudolabelled target samples based on the image classiﬁer and the last domain classiﬁer from the previous training epoch and re-train our CAN model by using the enlarged training set. Comprehensive experiments on two benchmark datasets Ofﬁce and ImageCLEF-DA clearly demonstrate the effectiveness of our newly proposed approaches CAN and iCAN for unsupervised domain adaptation.},
	language = {en},
	urldate = {2019-03-20},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Weichen and Ouyang, Wanli and Li, Wen and Xu, Dong},
	month = jun,
	year = {2018},
	pages = {3801--3809},
	file = {Zhang et al. - 2018 - Collaborative and Adversarial Network for Unsuperv.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/JYAEV9LV/Zhang et al. - 2018 - Collaborative and Adversarial Network for Unsuperv.pdf:application/pdf}
}

@inproceedings{hong_conditional_2018,
	address = {Salt Lake City, UT},
	title = {Conditional {Generative} {Adversarial} {Network} for {Structured} {Domain} {Adaptation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578243/},
	doi = {10.1109/CVPR.2018.00145},
	abstract = {In recent years, deep neural nets have triumphed over many computer vision problems, including semantic segmentation, which is a critical task in emerging autonomous driving and medical image diagnostics applications. In general, training deep neural nets requires a humongous amount of labeled data, which is laborious and costly to collect and annotate. Recent advances in computer graphics shed light on utilizing photo-realistic synthetic data with computer generated annotations to train neural nets. Nevertheless, the domain mismatch between real images and synthetic ones is the major challenge against harnessing the generated data and labels. In this paper, we propose a principled way to conduct structured domain adaption for semantic segmentation, i.e., integrating GAN into the FCN framework to mitigate the gap between source and target domains. Speciﬁcally, we learn a conditional generator to transform features of synthetic images to real-image like features, and a discriminator to distinguish them. For each training batch, the conditional generator and the discriminator compete against each other so that the generator learns to produce real-image like features to fool the discriminator; afterwards, the FCN parameters are updated to accommodate the changes of GAN. In experiments, without using labels of real image data, our method signiﬁcantly outperforms the baselines as well as state-of-the-art methods by 12\% ∼ 20\% mean IoU on the Cityscapes dataset. Syn.},
	language = {en},
	urldate = {2019-03-20},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Hong, Weixiang and Wang, Zhenzhen and Yang, Ming and Yuan, Junsong},
	month = jun,
	year = {2018},
	pages = {1335--1344},
	file = {Hong et al. - 2018 - Conditional Generative Adversarial Network for Str.pdf:/home/nicolai/.mozilla/firefox/8l6dt2sc.default/zotero/storage/WZQ4EQ62/Hong et al. - 2018 - Conditional Generative Adversarial Network for Str.pdf:application/pdf}
}

@article{murez_image_2017,
	title = {Image to {Image} {Translation} for {Domain} {Adaptation}},
	volume = {abs/1712.00479},
	url = {http://arxiv.org/abs/1712.00479},
	journal = {CoRR},
	author = {Murez, Zak and Kolouri, Soheil and Kriegman, David J. and Ramamoorthi, Ravi and Kim, Kyungnam},
	year = {2017}
}

@article{richter_playing_2016,
	title = {Playing for {Data}: {Ground} {Truth} from {Computer} {Games}},
	volume = {abs/1608.02192},
	url = {http://arxiv.org/abs/1608.02192},
	journal = {CoRR},
	author = {Richter, Stephan R. and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
	year = {2016}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@Manual{blender, 
   title = {Blender - a 3D modelling and rendering package}, 
   author = {Blender Online Community}, 
   organization = {Blender Foundation}, 
   address = {Stichting Blender Foundation, Amsterdam}, 
   year = {2018}, 
   url = {http://www.blender.org}, 
 }
 
 @misc{unreal,
  title = {Unreal Engine},
  howpublished = {\url{https://www.unrealengine.com/en-US/what-is-unreal-engine-4}},
  note = {Accessed: 2019-08-05}
}

@article{roy2019_semgan,
  author    = {Pravakar Roy and
               Nicolai H{\"{a}}ni and
               Volkan Isler},
  title     = {Semantics-Aware Image to Image Translation and Domain Transfer},
  journal   = {CoRR},
  volume    = {abs/1904.02203},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.02203},
  archivePrefix = {arXiv},
  eprint    = {1904.02203},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-02203},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hsvcolor,
  title={Color image segmentation: advances and prospects},
  author={Cheng, Heng-Da and Jiang, X\_ H\_ and Sun, Ying and Wang, Jingli},
  journal={Pattern recognition},
  volume={34},
  number={12},
  pages={2259--2281},
  year={2001},
  publisher={Elsevier}
}

@inproceedings{cohen2016indoor,
	title={Indoor-Outdoor 3D Reconstruction Alignment},
	author={Cohen, Andrea and Sch{\"o}nberger, Johannes L and Speciale, Pablo and Sattler, Torsten and Frahm, Jan-Michael and Pollefeys, Marc},
	booktitle={European Conference on Computer Vision},
	pages={285--300},
	year={2016},
	organization={Springer}
}

@article{bonanni20173,
	title={3-d map merging on pose graphs},
	author={Bonanni, Taigo Maria and Della Corte, Bartolomeo and Grisetti, Giorgio},
	journal={IEEE Robotics and Automation Letters},
	volume={2},
	number={2},
	pages={1031--1038},
	year={2017},
	publisher={IEEE}
}

@inproceedings{zhou2016fast,
	title={Fast global registration},
	author={Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
	booktitle={European Conference on Computer Vision},
	pages={766--782},
	year={2016},
	organization={Springer}
}

@article{rosell2009obtaining,
  title={Obtaining the three-dimensional structure of tree orchards from remote 2D terrestrial LIDAR scanning},
  author={Rosell, Joan R and Llorens, Jordi and Sanz, Ricardo and Arno, Jaume and Ribes-Dasi, Manel and Masip, Joan and Escol{\`a}, Alexandre and Camp, Ferran and Solanelles, Francesc and Gr{\`a}cia, Felip and others},
  journal={Agricultural and Forest Meteorology},
  volume={149},
  number={9},
  pages={1505--1515},
  year={2009},
  publisher={Elsevier}
}

@article{del2015georeferenced,
  title={Georeferenced Scanning System to Estimate the Leaf Wall Area in Tree Crops},
  author={del-Moral-Mart{\'\i}nez, Ignacio and Arn{\'o}, Jaume and Sanz, Ricardo and Masip-Vilalta, Joan and Rosell-Polo, Joan R and others},
  journal={Sensors},
  volume={15},
  number={4},
  pages={8382--8405},
  year={2015},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@inproceedings{wu2013towards,
  title={Towards linear-time incremental structure from motion},
  author={Wu, Changchang},
  booktitle={3DTV-Conference, 2013 International Conference on},
  pages={127--134},
  year={2013},
  organization={IEEE}
}

@inproceedings{newcombe2011kinectfusion,
  title={KinectFusion: Real-time dense surface mapping and tracking},
  author={Newcombe, Richard A and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  booktitle={Mixed and augmented reality (ISMAR), 2011 10th IEEE international symposium on},
  pages={127--136},
  year={2011},
  organization={IEEE}
}

@article{mur2017orb,
  title={Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras},
  author={Mur-Artal, Raul and Tard{\'o}s, Juan D},
  journal={IEEE Transactions on Robotics},
  volume={33},
  number={5},
  pages={1255--1262},
  year={2017},
  publisher={IEEE}
}

@article{medeiros2017modeling,
  title={Modeling dormant fruit trees for agricultural automation},
  author={Medeiros, Henry and Kim, Donghun and Sun, Jianxin and Seshadri, Hariharan and Akbar, Shayan Ali and Elfiky, Noha M and Park, Johnny},
  journal={Journal of Field Robotics},
  volume={34},
  number={7},
  pages={1203--1224},
  year={2017},
  publisher={Wiley Online Library}
}

@inproceedings{bowman2017probabilistic,
  title={Probabilistic data association for semantic slam},
  author={Bowman, Sean L and Atanasov, Nikolay and Daniilidis, Kostas and Pappas, George J},
  booktitle={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  pages={1722--1729},
  year={2017},
  organization={IEEE}
}

@article{kuhn1955hungarian,
  title={The Hungarian method for the assignment problem},
  author={Kuhn, Harold W},
  journal={Naval research logistics quarterly},
  volume={2},
  number={1-2},
  pages={83--97},
  year={1955},
  publisher={Wiley Online Library}
}

@article{kalman,
  title={An introduction to the Kalman filter},
  author={Welch, Greg and Bishop, Gary and others},
  year={1995}
}

@article{klt,
  title={Kanade-lucas-tomasi (klt) feature tracker},
  author={Suhr, Jae Kyu},
  journal={Computer Vision (EEE6503)},
  pages={9--18},
  year={2009}
}

@article{arjovsky2017wasserstein,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}


@inproceedings{gta,
  title={Playing for data: Ground truth from computer games},
  author={Richter, Stephan R and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
  booktitle={European conference on computer vision},
  pages={102--118},
  year={2016},
  organization={Springer}
}

@inproceedings{cordts2016cityscapes,
  title={The cityscapes dataset for semantic urban scene understanding},
  author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3213--3223},
  year={2016}
}

@inproceedings{zhao2017pyramid,
  title={Pyramid scene parsing network},
  author={Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2881--2890},
  year={2017}
}


@book{cnh_cnh_2019,
	title = {{CNH} {Industrial} {Autonomous} {Tractor}},
	url = {http://www.cnhindustrial.com/en-us/media/thedaythefarmchanged},
	author = {{CNH}},
	year = {2019},
	annote = {Accessed: 2019-02-11}
}

@book{tanimura_plant_2019,
	title = {Plant {Tape}},
	url = {http://www.taproduce.com/innovation/technology/},
	author = {{Tanimura} and Antle, \&},
	year = {2019},
	annote = {Accessed: 2019-02-11}
}

@book{weeding,
	title = {{Autonomous} {Vegetable} {Weeding} {Robot} {DINO}},
	url = {https://www.naio-technologies.com/en/agricultural-equipment/large-scale-vegetable-weeding-robot/},
	author = {{na}{\"{i}}{o} {technologies}},
	year = {2019},
	annote = {Accessed: 2019-08-11}
}

@inproceedings{shock2002automation,
  title={Automation of subsurface drip irrigation for crop research},
  author={Shock, Clinton C and Feibert, Erik BG and Saunders, Lamont D and Eldredge, Eric P},
  booktitle={Proc., World Congress of Computers in Agriculture and Natural Resources},
  year={2002},
  organization={Iguacu Falls Brazil}
}

@book{fertilizing,
	title = {{Yard} {Feeder} {Automatic} {Fertilization}},
	url = {https://www.yardfeeder.com/},
	author = {{GRO-Systems LLC}},
	year = {2019},
	annote = {Accessed: 2019-08-11}
}


@misc{anderson2013robotic,
  title={Robotic pesticide application},
  author={Anderson, Noel Wayne},
  year={2013},
  month=aug # "~6",
  publisher={Google Patents},
  note={US Patent 8,504,234}
}

@article{manetto2017improvements,
  title={Improvements in citrus packing lines to reduce the mechanical damage to fruit},
  author={Manetto, Giuseppe and Cerruto, E and Pascuzzi, S and Santoro, F},
  journal={Chemical Engineering Transactions},
  volume={58},
  pages={391--396},
  year={2017}
}

@book{gamaya_gamaya_2019,
	title = {Gamaya},
	url = {https://gamaya.com/},
	author = {{Gamaya}},
	year = {2019},
	annote = {Accessed: 2019-02-11}
}

@article{vadivambal2011applications,
  title={Applications of thermal imaging in agriculture and food industry—a review},
  author={Vadivambal, R and Jayas, Digvir S},
  journal={Food and bioprocess technology},
  volume={4},
  number={2},
  pages={186--199},
  year={2011},
  publisher={Springer}
}

@article{bachmann2013micro,
  title={Micro UAV based georeferenced orthophoto generation in VIS+ NIR for precision agriculture},
  author={Bachmann, Ferry and Herbst, Ruprecht and Gebbers, Robin and Hafner, Verena V},
  journal={ISPRS-International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  number={2},
  pages={11--16},
  year={2013}
}


@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}

@article{PengI17,
  author    = {Cheng Peng and
               Volkan Isler},
  title     = {Optimal Reconstruction with a Small Number of Views},
  journal   = {CoRR},
  volume    = {abs/1704.00085},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.00085},
  archivePrefix = {arXiv},
  eprint    = {1704.00085},
  timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PengI17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ostyakov2018seigan,
  title={SEIGAN: Towards Compositional Image Generation by Simultaneously Learning to Segment, Enhance, and Inpaint},
  author={Ostyakov, Pavel and Suvorov, Roman and Logacheva, Elizaveta and Khomenko, Oleg and Nikolenko, Sergey I},
  journal={arXiv preprint arXiv:1811.07630},
  year={2018}
}


@article{silwal2017design,
  title={Design, integration, and field evaluation of a robotic apple harvester},
  author={Silwal, Abhisesh and Davidson, Joseph R and Karkee, Manoj and Mo, Changki and Zhang, Qin and Lewis, Karen},
  journal={Journal of Field Robotics},
  volume={34},
  number={6},
  pages={1140--1159},
  year={2017},
  publisher={Wiley Online Library}
}


@inproceedings{xiong2018design,
  title={Design and Evaluation of a Novel Cable-Driven Gripper with Perception Capabilities for Strawberry Picking Robots},
  author={Xiong, Ya and From, Pal Johan and Isler, Volkan},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7384--7391},
  year={2018},
  organization={IEEE}
}

@book{abundant,
	title = {Abundant Robotics},
	url = {https://www.abundantrobotics.com/},
	author = {{Abundant Robotics}},
	year = {2019},
	annote = {Accessed: 2019-02-11}
}


@article{zaremba2014recurrent,
  title={Recurrent neural network regularization},
  author={Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1409.2329},
  year={2014}
}
